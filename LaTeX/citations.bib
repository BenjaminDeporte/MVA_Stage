@misc{girin_dynamical_2022,
	title = {Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}},
	shorttitle = {Dynamical {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2008.12595},
	doi = {10.48550/arXiv.2008.12595},
	abstract = {Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	month = jul,
	year = {2022},
	note = {arXiv:2008.12595},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pincus_approximate_1991,
	title = {Approximate entropy as a measure of system complexity.},
	volume = {88},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.88.6.2297},
	doi = {10.1073/pnas.88.6.2297},
	abstract = {Techniques to determine changing system complexity from data are evaluated. Convergence of a frequently used correlation dimension algorithm to a finite value does not necessarily imply an underlying deterministic model or chaos. Analysis of a recently developed family of formulas and statistics, approximate entropy (ApEn), suggests that ApEn can classify complex systems, given at least 1000 data values in diverse settings that include both deterministic chaotic and stochastic processes. The capability to discern changing complexity from such a relatively small amount of data holds promise for applications of ApEn in a variety of contexts.},
	language = {en},
	number = {6},
	urldate = {2025-01-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Pincus, S M},
	month = mar,
	year = {1991},
	pages = {2297--2301},
}

@article{delgado-bonal_approximate_2019,
	title = {Approximate {Entropy} and {Sample} {Entropy}: {A} {Comprehensive} {Tutorial}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {Approximate {Entropy} and {Sample} {Entropy}},
	url = {https://www.mdpi.com/1099-4300/21/6/541},
	doi = {10.3390/e21060541},
	abstract = {Approximate Entropy and Sample Entropy are two algorithms for determining the regularity of series of data based on the existence of patterns. Despite their similarities, the theoretical ideas behind those techniques are different but usually ignored. This paper aims to be a complete guideline of the theory and application of the algorithms, intended to explain their characteristics in detail to researchers from different fields. While initially developed for physiological applications, both algorithms have been used in other fields such as medicine, telecommunications, economics or Earth sciences. In this paper, we explain the theoretical aspects involving Information Theory and Chaos Theory, provide simple source codes for their computation, and illustrate the techniques with a step by step example of how to use the algorithms properly. This paper is not intended to be an exhaustive review of all previous applications of the algorithms but rather a comprehensive tutorial where no previous knowledge is required to understand the methodology.},
	language = {en},
	number = {6},
	urldate = {2025-01-20},
	journal = {Entropy},
	author = {Delgado-Bonal, Alfonso and Marshak, Alexander},
	month = jun,
	year = {2019},
	keywords = {approximate entropy, sample entropy, information theory, chaos theory},
	pages = {541},
}

@misc{PRML,
    title = {Pattern Recognition and Machine Learning},
    url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
    author = {C Bishop},
    year = {2006},
    note = {Accessed on Month Day, Year}
}

@misc{ProbabilisticMachineLearning,
    title = {Probabilistic Macine Learning Advanced Topics},
    url = {https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/},
    author = {K Murphy},
    year = {2023},
}

@misc{ProbabilisticGraphicalModels,
	title = {Probabilistic {Graphical} {Models}},
	url = {https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/},
	abstract = {Most tasks require a person or an automated system to reasonâ€”to reach conclusions based on available information. The framework of probabilistic graphical ...},
	language = {en-US},
    author = {Koller, Friedman},
	urldate = {2025-07-21},
	journal = {MIT Press},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02691},
	doi = {10.48550/ARXIV.1906.02691},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	urldate = {2025-07-21},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {9780262182539},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
}


@article{shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {0005-8580},
	url = {https://ieeexplore.ieee.org/document/6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
	number = {3},
	urldate = {2025-07-28},
	journal = {The Bell System Technical Journal},
	author = {Shannon, C. E.},
	month = jul,
	year = {1948},
	pages = {379--423},
}