\documentclass[twoside,11pt]{report}
% available document structure commands :
% Report: \part{}, \chapter{}, \section{}, \subsection{}, \subsubsection{}, \paragraph{}, \subparagraph{}.

\input{preamble}
\input{glossary}
\input{newcommands}

% files for bibliography
% \addbibresource{chapter/references.bib}
\addbibresource{chapter/citations.bib}
\addbibresource{chapter/citations_2.bib}


\title{Dynamical Variational Autoencoders :\\ discrete-time and continuous-time models.\\ Links to Stochastic calculus\\
\vspace{2cm}
{\Large{ENS Paris-Saclay, MVA}}}

\author{
Benjamin Deporte : \href{mailto:benjamin.deporte@ens-paris-saclay.fr}{benjamin.deporte@ens-paris-saclay.fr}% student 1
}

\date{August 2025}

\begin{document}

\everymath{\displaystyle}
\maketitle

% * means no number
\chapter*{Abstract}
This report describes a particular class of \glspl{vae} : the \glspl{dvae}. \glspl{dvae} are a specific class of models, adapted to the study of data sequences.
In \glspl{dvae}, the latent variables are structured themselves as a correlated set (usually a sequence also), aiming at encoding the temporal dimension of the data. 
We will review the general formulation of \glspl{dvae} and the detailed implementation of three models : extended Kalman filter, \gls{vrnn}, and \gls{gpvae}.

We then provide an overview of the information theory framework for data sequences -specifically some results on entropy rates- as an attempt to empirically  quantify the degree of "randomness" of data sequences.
In doing so, we can try and evaluate the expressiveness of \glspl{dvae}, and their relative performance with respect to other well-known models (such as \glspl{lstm}).

However, it is the theory of stochastic calculus that will provide us with great insights regarding the expressiveness of \glspl{dvae}.
First, we will see that the solution of a \gls{sde} is a Markov Process, that can quite naturally be expressed as a \gls{dvae}.
Furthermore, if the \gls{sde} is actually linear, we will see that its solution is actually a Gaussian Process, and fits naturally into the \gls{gpvae} model. 
All the associated results of the kernels theory (notably results on the spectral theory) then apply.

Armed with those results, we carry out some experiments, to demonstrate... SURPRISE.

The code is available at : \url{https://github.com/BenjaminDeporte/MVA_Stage}

\chapter*{Acknowledgements}



\newpage
\singlespacing
\tableofcontents

\newpage
\listoffigures

\part{Introduction}
    
    \include{chapter/subject_intro}
    \include{chapter/outline}
    \include{chapter/related_work}

\part{Dynamical Variational Autoencoders}

This part presents the general framework of \glspl{dvae}, after a reminder of the key notion of \textbf{D-separation}, which is key in \glspl{gpm}.
Three models are described in details:
\begin{itemize}
    \item \textbf{Deep Kalman Filter} : this model arises as the first evolution of the well-known Kalman Filter, with richer, \glspl{mlp} networks for the encoder and decoder.
    \item \textbf{Variational Recurrent Neural Network} : at the other end of the spectrum, \glspl{vrnn} provide the most expressive discrete-time formulation of the encoder and decoder.
We describe here a different implementation from the one described in \cite{girin_dynamical_2022}.
    \item \textbf{Gaussian Process Variational Auto Encoder} : in this model, the prior over the latent variables is no longer discrete, but is a \gls{gp}.
This allows for sampling data at irregular intervals. Another benefit is the use of richer kernel families to encode prior knowledge.
\end{itemize}

    \include{chapter/D-separation}
    \include{chapter/Dynamical_VAEs}
    \include{chapter/Deep_Kalman_Filter}
    \include{chapter/VRNN}
    \include{chapter/GPVAE}
    % \include{chapter/Torch_implementation_take_aways}
    % \include{chapter/Torch_implementation_take_aways_2}

\part{Stochastic Processes and stochastic calculus}

    \include{chapter/Entropy_and_randomness}
    \include{chapter/Stochastic_calculus}
    \include{chapter/stochastic_processes_basics}
    \include{chapter/Perspective_1}
    \include{chapter/Perspective_2}

\part{Experiments}

    \include{chapter/experiments}

\part{Conclusion and Discussion}

    \include{chapter/conclusions}

\part{Appendices}
\begin{appendices}
   \include{chapter/appendix} 
\end{appendices}

\clearpage

\printglossary
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography
\clearpage

\end{document}