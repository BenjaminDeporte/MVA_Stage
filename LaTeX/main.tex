\documentclass[twoside,11pt]{report}
% available document structure commands :
% Report: \part{}, \chapter{}, \section{}, \subsection{}, \subsubsection{}, \paragraph{}, \subparagraph{}.

\input{preamble}
\input{glossary}
\input{newcommands}
\input{theorems.tex}

% files for bibliography
% \addbibresource{chapter/references.bib}
\addbibresource{chapter/citations.bib}
\addbibresource{chapter/citations_2.bib}


\title{Dynamical Variational Autoencoders :\\ discrete-time and continuous-time models.\\ Links to stochastic calculus and stochastic differential equations\\
\vspace{2cm}
{\Large{ENS Paris-Saclay, MVA}}}

\author{
Benjamin Deporte : \href{mailto:benjamin.deporte@ens-paris-saclay.fr}{benjamin.deporte@ens-paris-saclay.fr}% student 1
}

\date{August 2025}

\begin{document}

\everymath{\displaystyle}
\maketitle
% * means no number
\chapter*{Abstract}

The \glspl{vae} are a well-known class of generative models, described in \cite{kingma_introduction_2019}. The original i.i.d.
assumption over the latent variables carry strong limitations when considering data sequences over time. A richer
class of models aim at solving this limitaion : the \glspl{dvae}. In \glspl{dvae}, the latent variables are structured themselves as a correlated set (usually a sequence also),
 aiming at encoding the temporal dimension of the data. 

The first part of the report is dedicated to the general study of \glspl{dvae}. A great survey is \cite{girin_dynamical_2022} and is the basis for this part. 
We review the general formulation of \glspl{dvae} and the detailed implementation of three models :  deep Kalman filter, \gls{vrnn}, and \gls{gpvae}. 
Along with this study, we also provide an overview of the information theory framework for data sequences -specifically some results on entropy rates-
 to empirically  quantify the degree of "randomness" (of, more accurately, uncertainty) of data sequences.


If a discrete-time setting of the latent variables is straightforward -for example, structuring the latent variables as a first-order Markov chain-, this setting carries limitations too. 
The first, and possibly most important one, being that the time interval between data samples (and therefore their associated latent variables) is, by default, constant. In practice, data measurements can be made 
with a changing frequency. A natural idea is then to structure the latent variables as a continuous-time process with "nice" properties : a Gaussian process \cite{rasmussen_gaussian_2008}. 
This idea is mentionned briefly in \cite{girin_dynamical_2022}, and described in more details in \cite{casale_gaussian_2018}, \cite{fortuin_gp-vae:_2020}, \cite{titsias_bayesian_2010} and \cite{zhu_markovian_2023}. 
This last paper has triggered in me a significant interest, as it links \gls{gpvae} with \glspl{sde}. The theory of stochastic calculus is fascinating, demanding, and fruitful.
I invested a significatant amount of time in its study, especially \cite{mouvement-brownien-calcul-ito} and \cite{sarkka_applied_2019}.
This has been quite a challenge given the time frame but proved extremely useful. We present some key results,
and added a longer reference at the end of this report.

We see that the solution of a \gls{sde} is a Markov process in which the evolution over time of the transition probability density is described by the Fokker-Plank-Kolmogorov equation. Integrating those between two
time stamps, produces a natural framework our first two discrete models : deep Kalman filter and \gls{vrnn}. Another well known 
use-case is diffusion models. In the case of a linear \gls{sde}, the solution is a Gaussian process. Conversely, for some appropriate mean and kernel functions, 
a Gaussian process can be represented as the solution of a linear \gls{sde}. This allows to use off the shelf smoothers (and filters), such as Kalman and \gls{rts}, that scale linearly.

The code is available at : \url{https://github.com/BenjaminDeporte/MVA_Stage}

\chapter*{Acknowledgements}



\newpage
\singlespacing
\tableofcontents

\newpage
\listoffigures

\part{Introduction}
    
    \include{chapter/subject_intro}
    \include{chapter/outline}
    \include{chapter/related_work}

%
%
%---------------------- DYNAMICAL VAE ---------------------------------------------
%
%
\part{Dynamical Variational Autoencoders}

This part presents the general framework of \glspl{dvae}. We start with a reminder of the key notion of \textbf{D-separation}, which is central in \glspl{gpm}.

Then, we describe three models in details:
\begin{itemize}
    \item \textbf{Deep Kalman Filter} : this model arises as the first evolution of the well-known Kalman Filter, with richer, \glspl{mlp} networks for the encoder and decoder.
    \item \textbf{Variational Recurrent Neural Network} : at the other end of the spectrum, \glspl{vrnn} provide the most expressive discrete-time formulation of the encoder and decoder.
We describe here a different implementation from the one described in \cite{girin_dynamical_2022}.
    \item \textbf{Gaussian Process Variational Auto Encoder} : in this model, the prior over the latent variables is no longer discrete, but is a \gls{gp}.
This allows for sampling data at irregular intervals. Another benefit is the use of richer kernel families to encode prior knowledge.
\end{itemize}

    \include{chapter/D-separation}
    \include{chapter/Dynamical_VAEs}
    \include{chapter/Deep_Kalman_Filter}
    \include{chapter/VRNN}
    \include{chapter/GPVAE}
    % \include{chapter/Torch_implementation_take_aways}
    % \include{chapter/Torch_implementation_take_aways_2}

% 
%
% ------- STOCHASTIC CALCULUS BASICS, SDE AND DVAE ---------------------------
%
%

\part{Notions on stochastic differential equations and their relationships to DVAEs}

This part intends to present a self-contained "survival kit" material on stochastic calculus, in order to highlight the relationships between \glspl{sde} and \glspl{dvae}.

NB : stochastic calculus is a full mathematical field in itself, and a more detailed presentation is located at the very end of this report. For a full study
of the matter, the interested reader will likely enjoy \cite{mouvement-brownien-calcul-ito}, \cite{sarkka_applied_2019}, and refer to \cite{cours-jf-legall}.

    \include{chapter/Entropy_and_randomness}
    \include{chapter/SC_introduction}
    \include{chapter/SC_SDE_results}
    % add chapter for links between SDEs and DVAEs
    \include{chapter/filter_smoother_gps}

\part{Experiments}

    \include{chapter/experiments}

\part{Conclusion and Discussion}

    \include{chapter/conclusions}

%
%
% ----- STOCHASTIC CALCULUS IN DETAILS ---------------------
%
%
\part{More on stochastic calculus}

    \include{chapter/stochastic_processes_basics}
    \include{chapter/stochastic_calculus}
    \include{chapter/ito_calculus_sde}

% \part{Appendices}
\begin{appendices}
   \include{chapter/appendix} 
\end{appendices}

\clearpage

\printglossary
\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\printbibliography
\clearpage

\end{document}