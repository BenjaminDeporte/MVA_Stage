@misc{mouvement-brownien-calcul-ito,
	title = {Mouvement brownien et calcul d'{Itô}-{Léonard} {Gallardo}-{Editions} {Hermann}},
	url = {https://www.editions-hermann.fr/livre/mouvement-brownien-et-calcul-d-ito-leonard-gallardo},
	abstract = {Le mouvement désordonné de particules de pollen en suspension dans un liquide en équilibre fut observé et rigoureusement rapporté par le botaniste écossais Robert Brown en 1827. 
Ce phénomène aléatoire lié à l'agitation moléculaire reçut par la suite le nom de mouvement brownien. Sa description mathématique comme un processus stochastique a captivé l'attention des physiciens et mathématiciens depuis plus d'un siècle. Il intervient dans de très nombreux modèles en physique, chimie, biologie, sciences économiques et mathématiques financières. 
Le mouvement brownien est l'objet central du calcul des probabilités moderne : il est tout à la fois une martingale, un processus gaussien, un processus à accroissements indépendants et un processus de Markov. Ces diverses propriétés qui en font le processus stochastique par excellence, sont présentées dans cet ouvrage avec les deux outils qu'il permet de développer : l'intégrale d'Itô et la notion d'équation différentielle stochastique. 
Ce livre s'adresse à tous ceux et celles qui recherchent une introduction rapide et rigoureuse aux méthodes du calcul stochastique, en particulier aux étudiants des master de mathématiques, aux élèves des grandes écoles scientifiques ainsi qu'aux candidats à l'agrégation. Nous y avons inclus des exercices de difficulté variée, corrigés en fin de volume, pour en faciliter la lecture et l'utilisation comme outil pédagogique.},
	urldate = {2025-04-16},
	journal = {Hermann},
	month = sep,
	year = {2008},
}

@misc{cours-jf-legall,
	title = {Page {Web} de {Jean}-{François} {Le} {Gall}},
	url = {https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/},
	abstract = {Page Web},
	urldate = {2025-04-16},
}

@book{bishop_pattern_2016,
	address = {New York, NY},
	edition = {Softcover reprint of the original 1st edition 2006 (corrected at 8th printing 2009)},
	series = {Information science and statistics},
	title = {Pattern {Recognition} and {Machine} {Learning}},
	isbn = {9781493938438},
	abstract = {The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners},
	language = {eng},
	publisher = {Springer New York},
	author = {Bishop, Christopher M.},
	year = {2016},
}

@misc{fortuin_gp-vae:_2020,
	title = {{GP}-{VAE}: {Deep} {Probabilistic} {Time} {Series} {Imputation}},
	shorttitle = {{GP}-{VAE}},
	url = {http://arxiv.org/abs/1907.04155},
	doi = {10.48550/arXiv.1907.04155},
	abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
	urldate = {2025-06-07},
	publisher = {arXiv},
	author = {Fortuin, Vincent and Baranchuk, Dmitry and Rätsch, Gunnar and Mandt, Stephan},
	month = feb,
	year = {2020},
	note = {arXiv:1907.04155},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{casale_gaussian_2018,
	title = {Gaussian {Process} {Prior} {Variational} {Autoencoders}},
	url = {https://arxiv.org/abs/1810.11738v2},
	abstract = {Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.},
	language = {en},
	urldate = {2025-06-07},
	journal = {arXiv.org},
	author = {Casale, Francesco Paolo and Dalca, Adrian V. and Saglietti, Luca and Listgarten, Jennifer and Fusi, Nicolo},
	month = oct,
	year = {2018},
}

@misc{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	doi = {10.48550/arXiv.1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2025-06-07},
	publisher = {arXiv},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv:1506.02216},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_gaussian_nodate,
	title = {Gaussian {Processes} for {Machine} {Learning}: {Book} webpage},
	url = {https://gaussianprocess.org/gpml/},
	urldate = {2025-06-07},
}

@article{roberts_gaussian_2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for
              Gaussian processes
              . We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2025-07-23},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
}

@inproceedings{titsias_bayesian_2010,
	address = {Chia Laguna Resort, Sardinia, Italy},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Bayesian {Gaussian} {Process} {Latent} {Variable} {Model}},
	volume = {9},
	url = {https://proceedings.mlr.press/v9/titsias10a.html},
	abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Titsias, Michalis and Lawrence, Neil D.},
	editor = {Teh, Yee Whye and Titterington, Mike},
	month = may,
	year = {2010},
	pages = {844--851},
}

@misc{li_disentangled_2018,
	title = {Disentangled {Sequential} {Autoencoder}},
	url = {http://arxiv.org/abs/1803.02991},
	doi = {10.48550/arXiv.1803.02991},
	abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Li, Yingzhen and Mandt, Stephan},
	month = jun,
	year = {2018},
	note = {arXiv:1803.02991},
	keywords = {Computer Science - Machine Learning},
}

@misc{zhu_markovian_2023,
	title = {Markovian {Gaussian} {Process} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2207.05543},
	doi = {10.48550/arXiv.2207.05543},
	abstract = {Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practioners. In this work, we leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional temporal and spatiotemporal tasks that our method performs favourably compared to existing approaches whilst being computationally highly scalable.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Zhu, Harrison and Rodas, Carles Balsells and Li, Yingzhen},
	month = aug,
	year = {2023},
	note = {arXiv:2207.05543},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@book{sarkka_applied_2019,
	address = {Cambridge},
	series = {Institute of {Mathematical} {Statistics} {Textbooks}},
	title = {Applied {Stochastic} {Differential} {Equations}},
	isbn = {9781316510087},
	url = {https://www.cambridge.org/core/books/applied-stochastic-differential-equations/6BB1B8B0819F8C12616E4A0C78C29EAA},
	abstract = {Stochastic differential equations are differential equations whose solutions are stochastic processes. They exhibit appealing mathematical properties that are useful in modeling uncertainties and noisy phenomena in many disciplines. This book is motivated by applications of stochastic differential equations in target tracking and medical technology and, in particular, their use in methodologies such as filtering, smoothing, parameter estimation, and machine learning. It builds an intuitive hands-on understanding of what stochastic differential equations are all about, but also covers the essentials of Itô calculus, the central theorems in the field, and such approximation schemes as stochastic Runge–Kutta. Greater emphasis is given to solution methods than to analysis of theoretical properties of the equations. The book's practical approach assumes only prior understanding of ordinary differential equations. The numerous worked examples and end-of-chapter exercises include application-driven derivations and computational assignments. MATLAB/Octave source code is available for download, promoting hands-on work with the methods.},
	urldate = {2025-07-23},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Solin, Arno},
	year = {2019},
	doi = {10.1017/9781108186735},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	title = {Elements of {Information} {Theory} 2nd {Edition}},
	isbn = {9780471241959},
	abstract = {The latest edition of this classic is updated with new problem sets and material   The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}