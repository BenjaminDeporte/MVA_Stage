@misc{girin_dynamical_2022,
	title = {Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}},
	shorttitle = {Dynamical {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2008.12595},
	doi = {10.48550/arXiv.2008.12595},
	abstract = {Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and Alameda-Pineda, Xavier},
	month = jul,
	year = {2022},
	note = {arXiv:2008.12595},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{pincus_approximate_1991,
	title = {Approximate entropy as a measure of system complexity.},
	volume = {88},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.88.6.2297},
	doi = {10.1073/pnas.88.6.2297},
	abstract = {Techniques to determine changing system complexity from data are evaluated. Convergence of a frequently used correlation dimension algorithm to a finite value does not necessarily imply an underlying deterministic model or chaos. Analysis of a recently developed family of formulas and statistics, approximate entropy (ApEn), suggests that ApEn can classify complex systems, given at least 1000 data values in diverse settings that include both deterministic chaotic and stochastic processes. The capability to discern changing complexity from such a relatively small amount of data holds promise for applications of ApEn in a variety of contexts.},
	language = {en},
	number = {6},
	urldate = {2025-01-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Pincus, S M},
	month = mar,
	year = {1991},
	pages = {2297--2301},
}

@article{delgado-bonal_approximate_2019,
	title = {Approximate {Entropy} and {Sample} {Entropy}: {A} {Comprehensive} {Tutorial}},
	volume = {21},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	shorttitle = {Approximate {Entropy} and {Sample} {Entropy}},
	url = {https://www.mdpi.com/1099-4300/21/6/541},
	doi = {10.3390/e21060541},
	abstract = {Approximate Entropy and Sample Entropy are two algorithms for determining the regularity of series of data based on the existence of patterns. Despite their similarities, the theoretical ideas behind those techniques are different but usually ignored. This paper aims to be a complete guideline of the theory and application of the algorithms, intended to explain their characteristics in detail to researchers from different fields. While initially developed for physiological applications, both algorithms have been used in other fields such as medicine, telecommunications, economics or Earth sciences. In this paper, we explain the theoretical aspects involving Information Theory and Chaos Theory, provide simple source codes for their computation, and illustrate the techniques with a step by step example of how to use the algorithms properly. This paper is not intended to be an exhaustive review of all previous applications of the algorithms but rather a comprehensive tutorial where no previous knowledge is required to understand the methodology.},
	language = {en},
	number = {6},
	urldate = {2025-01-20},
	journal = {Entropy},
	author = {Delgado-Bonal, Alfonso and Marshak, Alexander},
	month = jun,
	year = {2019},
	keywords = {approximate entropy, sample entropy, information theory, chaos theory},
	pages = {541},
}

@misc{PRML,
    title = {Pattern Recognition and Machine Learning},
    url = {https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf},
    author = {C Bishop},
    year = {2006},
    note = {Accessed on Month Day, Year}
}

@misc{ProbabilisticMachineLearning,
    title = {Probabilistic Machine Learning Advanced Topics},
    url = {https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/},
    author = {K Murphy},
    year = {2023},
}

@misc{ProbabilisticGraphicalModels,
	title = {Probabilistic {Graphical} {Models}},
	url = {https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/},
	abstract = {Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical ...},
	language = {en-US},
    author = {Koller, Friedman},
	urldate = {2025-07-21},
	journal = {MIT Press},
}

@article{kingma_introduction_2019,
	title = {An {Introduction} to {Variational} {Autoencoders}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1906.02691},
	doi = {10.48550/ARXIV.1906.02691},
	abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
	urldate = {2025-07-21},
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
}

@book{rasmussen_gaussian_2008,
	address = {Cambridge, Mass.},
	edition = {3. print},
	series = {Adaptive computation and machine learning},
	title = {Gaussian processes for machine learning},
	isbn = {9780262182539},
	language = {eng},
	publisher = {MIT Press},
	author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
	year = {2008},
}


@article{shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	issn = {0005-8580},
	url = {https://ieeexplore.ieee.org/document/6773024},
	doi = {10.1002/j.1538-7305.1948.tb01338.x},
	abstract = {The recent development of various methods of modulation such as PCM and PPM which exchange bandwidth for signal-to-noise ratio has intensified the interest in a general theory of communication. A basis for such a theory is contained in the important papers of Nyquist1 and Hartley2 on this subject. In the present paper we will extend the theory to include a number of new factors, in particular the effect of noise in the channel, and the savings possible due to the statistical structure of the original message and due to the nature of the final destination of the information.},
	number = {3},
	urldate = {2025-07-28},
	journal = {The Bell System Technical Journal},
	author = {Shannon, C. E.},
	month = jul,
	year = {1948},
	pages = {379--423},
}

@misc{kingma_auto-encoding_2022,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	doi = {10.48550/arXiv.1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions are two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	urldate = {2025-08-08},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Welling, Max},
	month = dec,
	year = {2022},
	note = {arXiv:1312.6114},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{mva_kernel_class,
	title = {Course materials: ({Slides}) - {Machine} learning with kernel methods / {Spring} 2025},
	url = {https://mva-kernel-methods.github.io/course-page/},
	abstract = {Course materials: (Slides) - Machine learning with kernel methods / Spring 2025},
	urldate = {2025-08-11},
}

@misc{mouvement-brownien-calcul-ito,
	title = {Mouvement brownien et calcul d'{Itô}-{Léonard} {Gallardo}-{Editions} {Hermann}},
	url = {https://www.editions-hermann.fr/livre/mouvement-brownien-et-calcul-d-ito-leonard-gallardo},
	abstract = {Le mouvement désordonné de particules de pollen en suspension dans un liquide en équilibre fut observé et rigoureusement rapporté par le botaniste écossais Robert Brown en 1827. 
Ce phénomène aléatoire lié à l'agitation moléculaire reçut par la suite le nom de mouvement brownien. Sa description mathématique comme un processus stochastique a captivé l'attention des physiciens et mathématiciens depuis plus d'un siècle. Il intervient dans de très nombreux modèles en physique, chimie, biologie, sciences économiques et mathématiques financières. 
Le mouvement brownien est l'objet central du calcul des probabilités moderne : il est tout à la fois une martingale, un processus gaussien, un processus à accroissements indépendants et un processus de Markov. Ces diverses propriétés qui en font le processus stochastique par excellence, sont présentées dans cet ouvrage avec les deux outils qu'il permet de développer : l'intégrale d'Itô et la notion d'équation différentielle stochastique. 
Ce livre s'adresse à tous ceux et celles qui recherchent une introduction rapide et rigoureuse aux méthodes du calcul stochastique, en particulier aux étudiants des master de mathématiques, aux élèves des grandes écoles scientifiques ainsi qu'aux candidats à l'agrégation. Nous y avons inclus des exercices de difficulté variée, corrigés en fin de volume, pour en faciliter la lecture et l'utilisation comme outil pédagogique.},
	urldate = {2025-04-16},
	journal = {Hermann},
	month = sep,
	year = {2008},
}

@misc{cours-jf-legall,
	title = {Page {Web} de {Jean}-{François} {Le} {Gall}},
	url = {https://www.imo.universite-paris-saclay.fr/~jean-francois.le-gall/},
	abstract = {Page Web},
	urldate = {2025-04-16},
}

@book{bishop-pattern-2016,
 	address = {New York, NY},
 	edition = {Softcover reprint of the original 1st edition 2006 (corrected at 8th printing 2009)},
 	series = {Information science and statistics},
 	title = {Pattern {Recognition} and {Machine} {Learning}},
 	isbn = {9781493938438},
 	abstract = {The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners},
 	language = {eng},
 	publisher = {Springer New York},
 	author = {Bishop, Christopher M.},
 	year = {2016},
 }

@misc{fortuin_gp-vae:_2020,
	title = {{GP}-{VAE}: {Deep} {Probabilistic} {Time} {Series} {Imputation}},
	shorttitle = {{GP}-{VAE}},
	url = {http://arxiv.org/abs/1907.04155},
	doi = {10.48550/arXiv.1907.04155},
	abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
	urldate = {2025-06-07},
	publisher = {arXiv},
	author = {Fortuin, Vincent and Baranchuk, Dmitry and Rätsch, Gunnar and Mandt, Stephan},
	month = feb,
	year = {2020},
	note = {arXiv:1907.04155},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{casale_gaussian_2018,
	title = {Gaussian {Process} {Prior} {Variational} {Autoencoders}},
	url = {https://arxiv.org/abs/1810.11738v2},
	abstract = {Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.},
	language = {en},
	urldate = {2025-06-07},
	journal = {arXiv.org},
	author = {Casale, Francesco Paolo and Dalca, Adrian V. and Saglietti, Luca and Listgarten, Jennifer and Fusi, Nicolo},
	month = oct,
	year = {2018},
}

@misc{chung_recurrent_2016,
	title = {A {Recurrent} {Latent} {Variable} {Model} for {Sequential} {Data}},
	url = {http://arxiv.org/abs/1506.02216},
	doi = {10.48550/arXiv.1506.02216},
	abstract = {In this paper, we explore the inclusion of latent random variables into the dynamic hidden state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN)1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamic hidden state.},
	urldate = {2025-06-07},
	publisher = {arXiv},
	author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
	month = apr,
	year = {2016},
	note = {arXiv:1506.02216},
	keywords = {Computer Science - Machine Learning},
}

@misc{noauthor_gaussian_nodate,
	title = {Gaussian {Processes} for {Machine} {Learning}: {Book} webpage},
	url = {https://gaussianprocess.org/gpml/},
	urldate = {2025-06-07},
}

@article{roberts_gaussian_2013,
	title = {Gaussian processes for time-series modelling},
	volume = {371},
	issn = {1364-503X, 1471-2962},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2011.0550},
	doi = {10.1098/rsta.2011.0550},
	abstract = {In this paper, we offer a gentle introduction to Gaussian processes for time-series data analysis. The conceptual framework of Bayesian modelling for time-series data is discussed and the foundations of Bayesian non-parametric modelling presented for
              Gaussian processes
              . We discuss how domain knowledge influences design of the Gaussian process models and provide case examples to highlight the approaches.},
	language = {en},
	number = {1984},
	urldate = {2025-07-23},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Roberts, S. and Osborne, M. and Ebden, M. and Reece, S. and Gibson, N. and Aigrain, S.},
	month = feb,
	year = {2013},
	pages = {20110550},
}

@inproceedings{titsias_bayesian_2010,
	address = {Chia Laguna Resort, Sardinia, Italy},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Bayesian {Gaussian} {Process} {Latent} {Variable} {Model}},
	volume = {9},
	url = {https://proceedings.mlr.press/v9/titsias10a.html},
	abstract = {We introduce a variational inference framework for training the Gaussian process latent variable model and thus performing Bayesian nonlinear dimensionality reduction. This method allows us to variationally integrate out the input variables of the Gaussian process and compute a lower bound on the exact marginal likelihood of the nonlinear latent variable model. The maximization of the variational lower bound provides a Bayesian training procedure that is robust to overfitting and can automatically select the dimensionality of the nonlinear latent space. We demonstrate our method on real world datasets. The focus in this paper is on dimensionality reduction problems, but the methodology is more general. For example, our algorithm is immediately applicable for training Gaussian process models in the presence of missing or uncertain inputs.},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Titsias, Michalis and Lawrence, Neil D.},
	editor = {Teh, Yee Whye and Titterington, Mike},
	month = may,
	year = {2010},
	pages = {844--851},
}

@misc{li_disentangled_2018,
	title = {Disentangled {Sequential} {Autoencoder}},
	url = {http://arxiv.org/abs/1803.02991},
	doi = {10.48550/arXiv.1803.02991},
	abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Li, Yingzhen and Mandt, Stephan},
	month = jun,
	year = {2018},
	note = {arXiv:1803.02991},
	keywords = {Computer Science - Machine Learning},
}

@misc{zhu_markovian_2023,
	title = {Markovian {Gaussian} {Process} {Variational} {Autoencoders}},
	url = {http://arxiv.org/abs/2207.05543},
	doi = {10.48550/arXiv.2207.05543},
	abstract = {Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practioners. In this work, we leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional temporal and spatiotemporal tasks that our method performs favourably compared to existing approaches whilst being computationally highly scalable.},
	urldate = {2025-07-23},
	publisher = {arXiv},
	author = {Zhu, Harrison and Rodas, Carles Balsells and Li, Yingzhen},
	month = aug,
	year = {2023},
	note = {arXiv:2207.05543},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@book{sarkka_applied_2019,
	address = {Cambridge},
	series = {Institute of {Mathematical} {Statistics} {Textbooks}},
	title = {Applied {Stochastic} {Differential} {Equations}},
	isbn = {9781316510087},
	url = {https://www.cambridge.org/core/books/applied-stochastic-differential-equations/6BB1B8B0819F8C12616E4A0C78C29EAA},
	abstract = {Stochastic differential equations are differential equations whose solutions are stochastic processes. They exhibit appealing mathematical properties that are useful in modeling uncertainties and noisy phenomena in many disciplines. This book is motivated by applications of stochastic differential equations in target tracking and medical technology and, in particular, their use in methodologies such as filtering, smoothing, parameter estimation, and machine learning. It builds an intuitive hands-on understanding of what stochastic differential equations are all about, but also covers the essentials of Itô calculus, the central theorems in the field, and such approximation schemes as stochastic Runge–Kutta. Greater emphasis is given to solution methods than to analysis of theoretical properties of the equations. The book's practical approach assumes only prior understanding of ordinary differential equations. The numerous worked examples and end-of-chapter exercises include application-driven derivations and computational assignments. MATLAB/Octave source code is available for download, promoting hands-on work with the methods.},
	urldate = {2025-07-23},
	publisher = {Cambridge University Press},
	author = {Särkkä, Simo and Solin, Arno},
	year = {2019},
	doi = {10.1017/9781108186735},
}

@book{cover_elements_2006,
	address = {Hoboken, N.J},
	title = {Elements of {Information} {Theory} 2nd {Edition}},
	isbn = {9780471241959},
	abstract = {The latest edition of this classic is updated with new problem sets and material   The Second Edition of this fundamental textbook maintains the book's tradition of clear, thought-provoking instruction. Readers are provided once again with an instructive mix of mathematics, physics, statistics, and information theory.  All the essential topics in information theory are covered in detail, including entropy, data compression, channel capacity, rate distortion, network information theory, and hypothesis testing. The authors provide readers with a solid understanding of the underlying theory and applications. Problem sets and a telegraphic summary at the end of each chapter further assist readers. The historical notes that follow each chapter recap the main points.  The Second Edition features: * Chapters reorganized to improve teaching * 200 new problems * New material on source coding, portfolio theory, and feedback capacity * Updated references  Now current and enhanced, the Second Edition of Elements of Information Theory remains the ideal textbook for upper-level undergraduate and graduate courses in electrical engineering, statistics, and telecommunications.},
	language = {English},
	publisher = {Wiley-Interscience},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}


@misc{peluchetti_infinitely_2020,
	title = {Infinitely deep neural networks as diffusion processes},
	url = {http://arxiv.org/abs/1905.11065},
	doi = {10.48550/arXiv.1905.11065},
	abstract = {When the parameters are independently and identically distributed (initialized) neural networks exhibit undesirable properties that emerge as the number of layers increases, e.g. a vanishing dependency on the input and a concentration on restrictive families of functions including constant functions. We consider parameter distributions that shrink as the number of layers increases in order to recover well-behaved stochastic processes in the limit of infinite depth. This leads to set forth a link between infinitely deep residual networks and solutions to stochastic differential equations, i.e. diffusion processes. We show that these limiting processes do not suffer from the aforementioned issues and investigate their properties.},
	urldate = {2025-09-05},
	publisher = {arXiv},
	author = {Peluchetti, Stefano and Favaro, Stefano},
	month = mar,
	year = {2020},
	note = {arXiv:1905.11065},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
}

@misc{chen_neural_2019,
	title = {Neural {Ordinary} {Differential} {Equations}},
	url = {http://arxiv.org/abs/1806.07366},
	doi = {10.48550/arXiv.1806.07366},
	abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
	urldate = {2025-09-05},
	publisher = {arXiv},
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	month = dec,
	year = {2019},
	note = {arXiv:1806.07366},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
}

@misc{rubanova_latent_2019,
	title = {Latent {ODEs} for {Irregularly}-{Sampled} {Time} {Series}},
	url = {http://arxiv.org/abs/1907.03907},
	doi = {10.48550/arXiv.1907.03907},
	abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
	urldate = {2025-09-05},
	publisher = {arXiv},
	author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
	month = jul,
	year = {2019},
	note = {arXiv:1907.03907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{li_scalable_2020,
	title = {Scalable {Gradients} for {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2001.01328},
	doi = {10.48550/arXiv.2001.01328},
	abstract = {The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset.},
	urldate = {2025-09-05},
	publisher = {arXiv},
	author = {Li, Xuechen and Wong, Ting-Kam Leonard and Chen, Ricky T. Q. and Duvenaud, David},
	month = oct,
	year = {2020},
	note = {arXiv:2001.01328},
	keywords = {Computer Science - Machine Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@book{pontriagin_mathematical_2018,
	address = {Boca Raton},
	series = {Classics of {Soviet} mathematics},
	title = {The mathematical theory of optimal processes},
	isbn = {9780203749319},
	language = {eng},
	publisher = {CRC Press},
	author = {Pontriagin, L S. and Boltianskiĭ, V G. and Gamkrelidze, R V. and Mishchenko, E F.},
	editor = {Neustadt, Lucien W.},
	year = {2018},
	note = {OCLC: 1035389999},
	keywords = {Mathematical optimization},
}

@article{sengupta_efficient_2014,
	title = {Efficient gradient computation for dynamical models},
	volume = {98},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811914003097},
	doi = {10.1016/j.neuroimage.2014.04.040},
	language = {en},
	urldate = {2025-09-07},
	journal = {NeuroImage},
	author = {Sengupta, B. and Friston, K.J. and Penny, W.D.},
	month = sep,
	year = {2014},
	pages = {521--527},
}