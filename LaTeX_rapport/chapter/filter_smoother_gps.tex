\chapter{Filtering, Smoothing, and the GP-VAE}\label{sec:filter smoother gps}

Equiped with the stochastic calculus basics, we see in this chapter that the filtering and smoothing 
tasks (ie computing posterior probabilities of the latent variables) provides a complete framework 
for the corresponding tasks in \glspl{dvae}.

We also see that, when a Gaussian process can be formulated as the solution of a linear \gls{sde} (ie
 when the kernel function verifies some properties), then the gaussian process regression problem 
 of computing posterior probabilities can be performed by algorithms in linear time.

In this chapter, we consider \glspl{ct-ssm} and \glspl{cd-ssm}. In both cases, the latent variables are 
defined by a (continuous) \gls{sde}. The observations can be defined by a second \gls{sde}, or by a set of 
discrete-time observations.

Formally, the \gls{ct-ssm} is defined by:

\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!black,title=Continuous-Time State Space model]
    \begin{align}
        dZ_t &= F(Z_t, t)dt + L(Z_t,t) dB_t \\
        dX_t &= H(Z_t,t)dt + d\eta_t
    \end{align}
    where:
    \begin{itemize}
        \item $Z_t \in \mathbb{R}^{D}$ is the \textit{state}, ie a stochastic process defining the latent variable.
        \item $B_t \in \mathbb{R}^{S}$ is a Brownian motion with diffusion matrix $Q$.
        \item $F \in \mathbb{R}^{D}$ and $L \in \mathbb{R}^{D \times S}$ are the usual drift and dispersion functions.
        \item $X_t \in \mathbb{R}^{M}$ is the \textit{integrated} measurement (or observation) process.
        \item $H \in \mathbb{R}^{M}$ is the observation/measurement model.
        \item $\eta_t \in \mathbb{R}^{S}$ is a Brownian motion with diffusion matrix $R$.
    \end{itemize}
    NB : the observations are assumed to conditionnally independent of the state, and $B_t, \eta_t$ are 
     assumed independent.
    The observation model is equivalent to:
    \begin{align}
        y_t &= \frac{dX_t}{dt} = H(Z_t,t) + \epsilon_t \\
        \epsilon_t &= \frac{d \eta_t}{dt}
    \end{align}
\end{tcolorbox}

Formally, the \gls{cd-ssm} is defined by:

\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!black,title=Continuous-Discrete State Space model]
    \begin{align}
        dZ_t &= F(Z_t, t)dt + L(Z_t,t) dB_t \\
        x_k &\sim p(x_k \vert z_{t_k})
    \end{align}
    where:
    \begin{itemize}
        \item $Z_t \in \mathbb{R}^{D}$ is the \textit{state}, ie a stochastic process defining the latent variable.
        \item $B_t \in \mathbb{R}^{S}$ is a Brownian motion with diffusion matrix $Q$.
        \item $F \in \mathbb{R}^{D}$ and $L \in \mathbb{R}^{D \times S}$ are the usual drift and dispersion functions.
        \item $x_k$ are the observations taken at \textbf{discrete times $(t_k)_{k=1,...,n}$}
    \end{itemize}
    NB : the observations are assumed to conditionnally independent of the state.
\end{tcolorbox}

We see that the \gls{gpvae} is a specific \gls{cd-ssm}, where the underlying latent stochastic process 
is actually a Gaussian process.

Also, the \gls{ct-ssm} assumes a Gaussian observation model, whereas the \gls{cd-ssm} allows more general 
observation models.

From a vocabulary stand-point, we will use indifferently \textit{state} or \textit{latent variable}, and 
\textit{observation} or \textit{measurement}.

\section{Filtering and Smooting}

\textbf{Filtering} is the problem of determining the posterior probability of the latent $Z_t$ given the 
discrete measurements \textbf{up to $t$}, ie finding $p(Z_t \vert x_{1:k})$ with $t_k \leq t$. This corresponds to 
determining the generative transition probability $p_{\theta_z}(z_t \vert z_{1:t-1}, x_{1:t-1})$ in our 
\gls{dvae} setting.

In general, close-form solutions can be derived when the latent variables \gls{sde} is linear. In continuous 
time, we get the \textbf{Kalman-Bucy} filter equations, which discretize in the well-known \textbf{Kalman filter}.

\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!black,title=Kalman-Bucy filter]
    \begin{align}
        dZ_t &= F(t)Z_t dt + L(t) dB_t \\
        dX_t &= H(t)X_t dt + d\eta_t
    \end{align}
    where:
    \begin{itemize}
        \item $Z_t \in \mathbb{R}^{D}$ is the state/latent.
        \item $X_t \in \mathbb{R}^{M}$ is the observation/measurement.
        \item $B_t \in \mathbb{R}^{S}$ is a Brownian motion with diffusion matrix $Q$.
        \item $\eta_t \in \mathbb{R}^{S}$ is a Brownian motion with diffusion matrix $R$.
        \item $F \in \mathbb{R}^{D}$ and $L \in \mathbb{R}^{D \times S}$ are the usual drift/dynamic model and dispersion functions.
        \item $H \in \mathbb{R}^{D \times M}$ is the measurement/observation model
    \end{itemize}
    NB : the observations are assumed to conditionnally independent of the state.
    Then the Bayesian filter (Kalman-Bucy) is:
    \begin{align}
        p(z_t \vert x_{<t}) &= \mathcal{N}(Z_t \vert m_t, P_t) \\
        K &= P H(t)^{T} R^{-1} \\
        dm &= F(t)m dt + K (dX_t - H(t) m dt) \\
        \frac{dP}{dt} &= F(t)P + P F(t)^{T} + L(t)QL(t)^{T} - KRK^{T}
    \end{align}
\end{tcolorbox}

In practice, one can approximate a general \gls{sde} by a linear \gls{sde} and apply Kalman-Bucy.

\textbf{Smoothing} is the problem of determining the posterior probability of the latent $Z_t$ given 
all known observations, ie finding $p(Z_t \vert x_{1:T})$ for all $t \in [0,T]$. This corresponds to 
determining the inference model $q_{\phi}(z_t \vert z_{1:t-1}, x_{1:T})$ in the \gls{dvae} setting.

We describe here briefly the \gls{rts} smoother for discrete time models.

Discretizing the transition density in \gls{cd-ssm}, we have
\begin{align}
    Z_{t_{k+1}} &\sim p(Z_{t_{k+1}} \vert Z_{t_k}) \\
    X_k &\sim p(X_k \vert Z_{t_k})
\end{align}

And the smoothers:

\begin{tcolorbox}[colback=blue!5!white,colframe=black!75!black,title=Bayesian smoother]
    \begin{align}
        Z_{t_{k+1}} &\sim p(Z_{t_{k+1}} \vert Z_{t_k}) \\
        X_k &\sim p(X_k \vert Z_{t_k})
    \end{align}
    The \textit{Bayesian smoother} is, for any $k < T$:
    \begin{align}
        p(Z_{t_{k+1}} \vert X_{1:k}) &= \int p(Z_{t_{k+1}} \vert Z_{t_k}) p(Z_{t_k} \vert X_{1:k}) dZ_{t_k} \\
        p(Z_{t_k} \vert X_{1:T}) &= p(Z_{t_{k}} \vert X_{1:k})\int \left(
            \frac{
                p(Z_{t_{k+1}} \vert Z_{t_k}) p(Z_{t_{k+1}} \vert X_{1:T})
            }{
                p(Z_{t_{k+1}} \vert X_{1:k}
            }dZ_{t_{k+1}}
        \right)
    \end{align}
    The backward recursion is started from the final step, where the filtering and smoothing densities 
    are the same : $p(Z_{t_T} \vert X_{1:T})$.
\end{tcolorbox}

The \gls{rts} smoother is the close-form solution of the Bayesian filter for a linear Gaussian problem 
- see \cite{sarkka_applied_2019} for the algorithm.

\section{GP-VAE}

We wrap up here linking the filtering/smoothing theory of linear \gls{sde} with the \gls{gpvae} 
model of \cite{fortuin_gp-vae:_2020}.

If we use the formalization above, a \gls{gpvae} is basically:

\begin{align}
    \label{gpvae sde form}
    Z_t &\sim \mathcal{GP}(m(\bullet), k(\bullet, \bullet)) \\
    X_{t_k} &\sim p(X_{t_k} \vert Z_{t_k})
\end{align}

If we assume that the observation model is Gaussian, then we get
\begin{align}
    \label{gpvae gaussian observation}
    Z_t &\sim \mathcal{GP}(m(\bullet), k(\bullet, \bullet)) \\
    X_{t_k} &\sim \mathcal{N}(X_{t_k} \vert Z_{t_k}, \sigma^{2})
\end{align}

Computing the posterior distribution $p(Z_t \vert X_{t_1:t_T})$ is performing a Gaussian Process 
regression (see \cite{rasmussen_gaussian_2008}), which naively scales in $O(n^{3})$.

However, if the Gaussian process can be written as a linear \gls{sde}:
\begin{align}
    \label{gpvae linear sde form}
    dZ_t &= F(t)Z_t dt + L(t)dB_t \\
    X_{t_k} &\sim \mathcal{N}(X_{t_k} \vert Z_{t_k}, \sigma^{2})
\end{align}

then the Kalman filter and smoother apply, that scale in $O(n)$. This is the main idea in \cite{zhu_markovian_2023}.

However - the solutions of linear \gls{sde} are Gaussian Processes, but the converse is not true. 
More specifically, some kernel functions are such that the associated \gls{gp} can not be represented as 
the solution of a linear \gls{sde}.

For a given kernel function $k(t,t')$, \cite{sarkka_applied_2019} aims at finding a linear time-invariant model
\begin{align}
    dZ_t &= F Z_t dt + L dB_t \\
    X_{t_k} &\sim \mathcal{N} (X_{t_k} \vert H Z_{t_k} , \sigma^{2})
\end{align}
with $Z_t \in  \mathbb{R}^{D}$, but $X_t \in \mathbb{R}$ is one-dimensional (ie $H \in \mathbb{R}^{1 \times D}$), 
and such that $z_t = H Z_t$ is a Gaussian process with kernel $k$.

We give now some examples, and counter-examples, of such associations.

\begin{itemize}
    \item \textbf{Brownian motion} : the Brownian motion is the solution of $dZ_t = dB_t$, 
    and a \gls{gp} with kernel $k(t,t') = \text{min}(t,t')$ (see \ref{sec:brownian_motion_gaussian_and_markov})
    \item \textbf{Ornstein Uhlenbeck} : the O.U. process
    \begin{align}
        dZ_t &= - \frac{1}{l} Z_t dt + dB_t
    \end{align}
    where $dB_t$ has diffusion coefficient $\frac{2 \sigma^{2}}{l}$, is a \gls{gp} with kernel:
    \begin{align}
        k_{\text{exp}} &= \sigma^{2} \exp{(- \frac{\vert t-t' \vert}{l})}
    \end{align}
    \item \textbf{Matern} : the \gls{sde} representation with
    \begin{align}
        F &= \begin{pmatrix}
            0 & 1 \\
            -\lambda^{2} & -2 \lambda 
        \end{pmatrix} \\
        L &= \begin{pmatrix}
            0 \\ 1
        \end{pmatrix} \\
        H &= \begin{pmatrix}
            1 \\ 0
        \end{pmatrix}
    \end{align}
    is a \gls{gp} with the Matern kernel with $\nu = \frac{3}{2}$:
    \begin{align}
        k_{\text{Matern}} &= \sigma^{2} \left(
            1 + \frac{\sqrt{3} \vert t-t' \vert}{l}
        \right) \exp{
            \left(
                -\frac{\sqrt{3} \vert t-t' \vert}{l}
            \right)
        }
    \end{align}
    and $\lambda = \frac{\sqrt{3}}{l}$, diffusion is $q = 4\lambda^{3}\sigma^{2}$.
\end{itemize}

Conversely, the following kernels can not be used to derive an associated linear \gls{sde}:

\begin{itemize}
    \item \textbf{squared exponential} : the widely used
    \begin{align}
        k_{\text{se}}(t,t') &= \sigma^{2} \exp{\left(
            - \frac{\vert t-t' \vert^{2}}{2l^{2}}
        \right)}
    \end{align}
    \item \textbf{rational quadratic}:
    \begin{align}
        k_{\text{rq}}(t,t') &= \sigma^{2} \left(
            1 + \frac{\vert t-t' \vert^{2}}{2 \alpha l^{2}}
        \right)^{-\alpha}
    \end{align}
    with $\alpha > 0$.
\end{itemize}

In that latter case, one can use spectral decomposition (ie Mercer's theorem, see MVA kernel class \cite{mva_kernel_class}) 
to approximate the kernel function and determine an associated linear \gls{sde}.