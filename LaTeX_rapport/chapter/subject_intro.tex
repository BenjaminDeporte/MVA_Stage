% \chapter{Subject}\label{sec:Subject}

% * means no number
% \chapter*{Abstract}

\glspl{vae} are a well-known class of generative models, described in \cite{kingma_introduction_2019}, which have spawn numerous applications. 
However, \glspl{vae} posit an i.i.d. assumption over the latent variables, that carries strong limitations if considering data sequences over time, 
where correlations often exist between data samples. A richer class of models aims at solving this limitation : the \glspl{dvae}.
In \glspl{dvae}, the latent variables are structured themselves as a correlated set (usually a sequence also), 
aiming at encoding the temporal behavior of the data. The relationship between the latent variables and the observed data (ie, the likelihood, 
or observation model, or decoder) remains as in regular \glspl{vae}.

The first part of the report is dedicated to the general study of \glspl{dvae}. We rely on the exhaustive survey \cite{girin_dynamical_2022},
which is the basis for this part. We review the general formulation of \glspl{dvae}, and review the detailed implementation of two discrete time models:
 the \gls{dkf}, and the \gls{vrnn}. A discrete-time setting of the latent variables is straightforward : for example, 
 structuring the latent variables as a first-order Markov chain leads to the general framework of \glspl{state-space-model}.
 However, this setting carries limitations too. The first limitation, and possibly most important one, 
 is that the structure of the model requires regularly-sampled data. In practice, data can be observed with a changing frequency. 
 A natural idea is then to define the latent variables prior as a continuous-time process, which will be sampled if and when 
 required to match an irregularly sampled data. A candidate for such a continuous-time process, endowed with convenient properties, 
 is the \gls{gp} \cite{rasmussen_gaussian_2008}. We review the corresponding \gls{dvae} : the \gls{gpvae} model.
 This model is only briefly mentionned in \cite{girin_dynamical_2022} (where the focus is on discrete time models), 
 but described in more details in \cite{casale_gaussian_2018}, \cite{fortuin_gp-vae:_2020}, \cite{titsias_bayesian_2010} 
 and \cite{zhu_markovian_2023}. 

The second part describes the implementations of those three models, the experiments run on toy datasets, the PyTorch tricks 
that sometimes cost quite a bit to learn (...), and the take-aways. A repository with the set of code and notebooks can be found 
at \href{https://github.com/BenjaminDeporte/MVA_Stage}{Benjamin's GitHub repo}.

One of the \gls{gpvae} papers, specifically \cite{zhu_markovian_2023}, has triggered a significant interest in me, as it showed a 
deep relationship between \glspl{dvae} and stochastic calculus.

I then invested a significant amount of time in studying the mathematical machinery required to fully understand this relationship. 
This steered me a little bit off course the subject per say, but the theory of stochastic calculus proved fascinating and commendable.
 Among good study books, are \cite{mouvement-brownien-calcul-ito} and \cite{sarkka_applied_2019}. Getting up to speed on stochastic calculus 
 has been quite a challenge given the time frame but proved extremely useful. We present some key results, which the knowledgeable reader 
 on stochastic calculus can skip, and added a longer reference at the end of this report. The aficionado of generative models will recognize 
the mathematical framework of diffusion models.

A first relationship between \glspl{dvae} and stochastic calculus is as follows: the solution of a general \gls{sde} is a Markov process (where 
the evolution over time of the transition probability density is described by the Fokker-Plank-Kolmogorov equation). This Markov process, 
when discretized, leads to the natural framework of our first two discrete models : the \gls{dkf} and \gls{vrnn}. Moreover, 
in the case of a linear \gls{sde}, the solution of the \gls{sde} is also a Gaussian process, which points to \gls{gpvae}. For solutions of linear \gls{sde}, 
one can use off the shelf smoothers (and filters), such as Kalman and \gls{rts}, that scale linearly instead of cubically as is usually the case 
for \glspl{gp} (\cite{zhu_markovian_2023}, \cite{sarkka_applied_2019})

However, the \gls{gpvae} model carries limitations too. First, some Gaussian processes -more specifically some kernel functions- 
can not be formulated as the solution to a linear \gls{sde}. We review quickly some such kernels. Second, some stochastic processes 
are both Markov processes and Gaussian processes (the Brownian motion itself, but also the well-studied Ornstein Uhlenbeck process), 
but some Markov processes (ie solutions to general \gls{sde}) are not Gaussian processes. 

An idea is then to use a more general stochastic process, defined as the solution to a general (ie non-linear) \gls{sde}, as the model for 
latent variables. The drift and the diffusion can also be learnt as neural networks. This is the field of \glspl{latent sde} (\cite{peluchetti_infinitely_2020}), 
which is a recent research field, and builds upon the \glspl{latent ode} (\cite{chen_neural_2019}, \cite{rubanova_latent_2019}) framework. 
We present some preliminary insights regarding \glspl{latent ode} and \glspl{latent sde}, however we have lacked time to go much further... for now.

It's been quite a journey...