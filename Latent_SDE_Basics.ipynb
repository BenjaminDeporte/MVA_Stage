{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80222970",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 28px; color: black; font-weight: bold;\">\n",
    "Basics on Neural Latent SDE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38425f08",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Biblio\n",
    "</div>\n",
    "\n",
    "Neural ODEs:\n",
    "\n",
    "**Neural ODEs (https://arxiv.org/abs/1806.07366) (2019)** : introduction of the Neural ODE as the continuous-time limit of a ResNet stack. Presentation of the use of the adjoint sensitivity method. Seminal paper for Neural ODE.\n",
    "\n",
    "**Latent ODEs for Irregularly-Sampled Time Series (https://arxiv.org/abs/1907.03907) (2019)** : Evolution of the Neural ODE model towards a Neural ODE RNN model, where the approximate posterior is built with a RNN on past observations.\n",
    "\n",
    "Neural SDEs:\n",
    "\n",
    "**SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations (https://arxiv.org/abs/2502.02472 , 2025)** : good background section (#2) to explain Neural SDE. Propose a new method SDE matching, inspired by score and flow matching, vs the adjoint sensivity method. SDE matching is claimed to be more efficient to compute gradients and train latent SDEs.\n",
    "\n",
    "**Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020)** : generalization of the adjoint sensitivity method to SDEs. Combination with gradient-based stochastic variational inference for infinite-dimension VAEs.\n",
    "\n",
    "**Neural SDEs (https://www.researchgate.net/publication/333418188_Neural_Stochastic_Differential_Equations) (2019)** : link between infinitely deep residual networks and solutions to stochastic differential equations\n",
    "\n",
    "**Stable Neural SDEs in analyzing irregular time series data (https://arxiv.org/abs/2402.14989) (2025)** : points to the necessity of careful design of the drift and diffusion neural nets in latent SDEs. Introduces three latent SDEs models with performance guarantees.\n",
    "\n",
    "**Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024)** : application of neural SDEs to a biological use case (brain activity). Details the model, architecture, ELBO/loss computation. Takes into account inputs/commands in the model. \n",
    "\n",
    "General/Misc:\n",
    "\n",
    "**Efﬁcient gradient computation for dynamical models (https://www.fil.ion.ucl.ac.uk/~wpenny/publications/efficient_revised.pdf) (2014)** : summary of finite difference method, forward sensitivity method, adjoint sensitivity method, to compute gradients of a functional cost function. Applies to Neural ODEs training.\n",
    "\n",
    "**Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing (https://arxiv.org/abs/1903.10145) (2019)** : explanation of the posterior collapse/KL vanishing problem, introduces different KL annealing schedules for VAE training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40944",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Model & Math\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4507903",
   "metadata": {},
   "source": [
    "Data : $\\mathbf{X} = (x_{t_1}, x_{t_2}, ..., x_{t_N}) \\in \\mathbb{R}^{D_x}$ - assuming all $t_i \\in [0,1]$.\n",
    "\n",
    "The latent space has dimension $D_z$. The latent continuous dynamic is $\\mathbf{Z}$ defined by:\n",
    "\\begin{align*}\n",
    "z_0^{(\\theta)} &\\sim p_{\\theta_z}(z_0) \\\\\n",
    "dz_t^{(\\theta)} &= f_{\\theta}(z_t, t)dt + \\sigma_{\\theta}(z_t,t)dB_t \n",
    "\\end{align*}\n",
    "with: \n",
    "\\begin{align}\n",
    "\\textbf{drift} \\,& f_{\\theta} : \\mathbb{R}^{D_z} \\times [0,1] \\rightarrow \\mathbb{R}^{D_z} \\\\\n",
    "\\textbf{diffusion} \\,& \\sigma_{\\theta} : \\mathbb{R}^{D_z} \\times [0,1] \\rightarrow \\mathbb{R}^{D_z \\times D_z} \\\\\n",
    "\\textbf{Brownian motion} \\,& dB_t \\in \\mathbb{R}^{D_z}\n",
    "\\end{align}\n",
    "\n",
    "The decoder is classically:\n",
    "\\begin{align}\n",
    "p_{\\theta_x}(x_{t_i} \\vert z_{t_i})\n",
    "\\end{align}\n",
    "\n",
    "The approximate posterior (encoder) is also a SDE:\n",
    "\\begin{align}\n",
    "z_0^{(\\phi)} &\\sim q_{\\phi}(z_0 \\vert \\textbf{X}) \\\\\n",
    "dz_t^{(\\phi)} &= f_{\\phi}(z_t, t, \\textbf{X})dt + \\sigma_{\\theta}(z_t,t)dB_t \n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- the drift $f_{\\phi}(z_t, t, \\textbf{X})$ is conditionned on observations $\\textbf{X}$\n",
    "- the diffusion of the approximate posterior is shared with the diffusion of the prior : $\\sigma_{\\theta}(z_t,t)$ - this ensures the application of Girsanov theorem and a finite KL divergence between the two stochastic processes (prior and approximate posterior) (see Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024))\n",
    "- drift and diffusion neural nets do not exhibit the same convergence guarantee (Stable Neural SDEs in analyzing irregular time series data (https://arxiv.org/abs/2402.14989) (2025))\n",
    "- non-diagonal diffusion seems to be difficult to simulate and costly to approximate (Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020))\n",
    "- it seems a good practice to encode only part of the $\\textbf{X}$ in the approximate posterior : context vector (Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020)), and $t_c << t_n$ in Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b7a86",
   "metadata": {},
   "source": [
    "Variational lower bound on the log marginal likelihood:\n",
    "\n",
    "We write:\n",
    "\\begin{align}\n",
    "p(x_{t_1:t_N}) &= \\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{p(z_{t_1:t_N} \\vert x_{t_1:t_N})}\n",
    "\\end{align}\n",
    "And:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}\\frac{q_{\\phi}(z\\vert X)}{p(z_{t_1:t_N} \\vert x_{t_1:t_N})}} dz\n",
    "\\end{align}\n",
    "where $q_{\\phi}(z \\vert X)$ is formally is posterior distribution over **functions** $z : \\mathbb{R} \\rightarrow \\mathbb{R}^{D_z}$.\n",
    "Then:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}} dz + \\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N} \\vert x_{t_1:t_N}))\n",
    "\\end{align}\n",
    "where we -audaciously- consider $p(z_{t_1:t_N} \\vert x_{t_1:t_N})$ as a dsitribution over functions $z$ taking values $z_{t_1:t_N}$ at times $t_1:t_N$ so the $\\mathbb{KL}$ actually means something.\n",
    "Still on the same path:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &\\geq \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}} dz \\\\\n",
    "&= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N} \\vert z_{t_1:t_N})}{q_{\\phi}(z\\vert X)} p(z_{t_1:t_N})} dz \\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(z \\vert X)} \\log{p(x_{t_1:t_N} \\vert z_{t_1:t_N})} - \\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N})) \\\\\n",
    "\\end{align}\n",
    "We write -still audaciously-\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N})) &= \\mathbb{KL}(q_{\\phi}(z_0\\vert X) \\vert\\vert p_{\\theta_z}(z_0)) + \\mathbb{KL}(q_{\\phi}(z_{>0}\\vert X) \\vert\\vert p_{\\theta_z}(z_{>0}))\n",
    "\\end{align}\n",
    "where the first $\\mathbb{KL}$ on the r.h.s is a classic between two probability distributions over a random variable, and the second is derived from the Girsanov's theorem as:\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(q_{\\phi}(z_{>0}\\vert X) \\vert\\vert p_{\\theta_z}(z_{>0})) &= \\frac{1}{2} \\mathbb{E}_{q_{\\phi}(z_{>0}\\vert X)} \\left( \\int_{0}^{T} \\vert \\Delta(t) \\vert^2 dt \\right) \\\\\n",
    "\\Delta(t) &= \\sigma_{\\theta}^{-1}(z_t,t) (f_{\\phi}(z_t, t, \\textbf{X}) - f_{\\theta}(z_t, t))\n",
    "\\end{align}\n",
    "\n",
    "Finally:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi, \\textbf{X}) &= \\mathbb{E}_{q_{\\phi}(z \\vert X)} \\log{p(x_{t_1:t_N} \\vert z_{t_1:t_N})} - \\mathbb{KL}(q_{\\phi}(z_0\\vert X) \\vert\\vert p_{\\theta_z}(z_0)) - \\frac{1}{2} \\mathbb{E}_{q_{\\phi}(z_{>0}\\vert X)} \\left( \\int_{0}^{T} \\vert \\Delta(t) \\vert^2 dt \\right)\n",
    "\\end{align}\n",
    "\n",
    "During training:\n",
    "- the integral is approximated via numerical integration\n",
    "- expectations are estimated with MC sampling\n",
    "- NB : sampling is actually : sampling $z_0 \\sim q_{\\phi}(z_0 \\vert \\textbf{X})$ and sampling a function $z$ by sampling a Brownian motion path $B_t$ and computing the whole realization path $z_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d885ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Code\n",
    "</div>\n",
    "\n",
    "https://github.com/google-research/torchsde\n",
    "\n",
    "[1] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud. \"Scalable Gradients for Stochastic Differential Equations\". International Conference on Artificial Intelligence and Statistics. 2020. [arXiv]\n",
    "\n",
    "[2] Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, Terry Lyons. \"Neural SDEs as Infinite-Dimensional GANs\". International Conference on Machine Learning 2021. [arXiv]\n",
    "\n",
    "[3] Patrick Kidger, James Foster, Xuechen Li, Terry Lyons. \"Efficient and Accurate Gradients for Neural SDEs\". 2021. [arXiv]\n",
    "\n",
    "[4] Patrick Kidger, James Morrill, James Foster, Terry Lyons, \"Neural Controlled Differential Equations for Irregular Time Series\". Neural Information Processing Systems 2020. [arXiv]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4bf9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Basic manipulations of the torchsde library\n",
    "</div>\n",
    "\n",
    "See also https://github.com/google-research/torchsde/blob/master/examples/demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915d2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsde\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828eb904",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f28af6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Creating a SDE model, sampling paths\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ab9846",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an Ornstein Uhlenbeck SDE model:\n",
    "# dX_t = theta * (mu - X_t) dt + sigma dW_t\n",
    "# theta, mu, sigma scalar parameters\n",
    "# W_t is a standard 1D Brownian motion\n",
    "\n",
    "# SDE are instantiated as subclasses of nn.Module\n",
    "\n",
    "class OUSDE(nn.Module):\n",
    "    def __init__(self, theta, mu, sigma):\n",
    "        \n",
    "        # noise type can take 4 values : \"diagonal\", \"general\", \"additive\", \"scalar\"\n",
    "        # here we use \"diagonal\" : the diffusion function g(t,y) is an element wise function,\n",
    "        # its output has the same shape as y, ie (batch_size, state_size)\n",
    "        \n",
    "        # sde_type can be \"ito\" or \"stratonovich\"\n",
    "        # we use \"ito\" here. The available methods for computation are Euler(-Maruyama), Milstein, SRK.\n",
    "        super().__init__()\n",
    "        self.noise_type = \"diagonal\"\n",
    "        self.sde_type = \"ito\"\n",
    "        \n",
    "        # we register the parameters so we can save them. But we will not train them.\n",
    "        self.register_buffer(\"theta\", torch.tensor(theta))\n",
    "        self.register_buffer(\"mu\", torch.tensor(mu))\n",
    "        self.register_buffer(\"sigma\", torch.tensor(sigma))\n",
    "\n",
    "    # DRIFT FUNCTION\n",
    "    # inputs are:\n",
    "    # - t : a tensor of shape (1,) representing the time stamps\n",
    "    # - y : a tensor of shape (batch_size, state_size) representing the current state\n",
    "    # outputs:\n",
    "    # - a tensor of shape (batch_size, state_size) representing the drift at time t and state y\n",
    "    # note : the functions f and g must be able to handle inputs of shape (batch_size, state_size)\n",
    "    # for any batch_size >= 1\n",
    "    def f(self, t, y):\n",
    "        return self.theta * (self.mu - y)\n",
    "    \n",
    "    # DIFFUSION FUNCTION\n",
    "    # inputs are:\n",
    "    # - t : a tensor of shape (1,) representing the time stamps\n",
    "    # - y : a tensor of shape (batch_size, state_size) representing the current state\n",
    "    # outputs:\n",
    "    # - a tensor of shape (batch_size, state_size) representing the diffusion at time t and state y\n",
    "    # (NB : generally, the output of g is of shape (batch_size, state_size, brownian_size) when noise_type is \"general\")\n",
    "    def g(self, t, y):\n",
    "        return self.sigma * torch.ones_like(y)\n",
    "    \n",
    "model = OUSDE(theta=2.0, mu=0.5, sigma=0.1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0233e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 100 # number of points in each path\n",
    "N_PATHS = 100  # number of paths to sample\n",
    "\n",
    "t_start = 0.0\n",
    "t_end = 3.0\n",
    "\n",
    "ts = torch.linspace(t_start, t_end, N_POINTS).to(device)  # time stamps where we want the solution, between 0 and 1\n",
    "print(f\"Time stamps shape : {ts.shape}\")  # shape (N_POINTS,)\n",
    "y0 = torch.full((N_PATHS, 1), 0.1).to(device)  # initial condition 0.1, shape (batch_size=N_PATHS, state_size=1)\n",
    "print(f\"Initial condition shape : {y0.shape}\")  # shape (N_PATHS, 1)\n",
    "\n",
    "# now, we call sdeint to solve the SDE\n",
    "# NB : we can use the adjoint method by calling sdeint_adjoint instead of sdeint\n",
    "# method can be \"euler\", \"milstein\", \"srk\" for sde_type=\"ito\"\n",
    "# dt is the step size used by the solver (smaller dt -> more accurate but slower). By default, dt=1e-3\n",
    "\n",
    "with torch.no_grad():  # we don't need gradients for this demo\n",
    "    ys = torchsde.sdeint(model, y0, ts, method=\"euler\", dt=1e-3)  # shape (N_POINTS, N_PATHS, 1)\n",
    "print(f\"Computed solution samples : {ys.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ousde_samples(ts, ys, model):\n",
    "    \"\"\"\n",
    "    Utility functions to plot the sampled SDE solutions\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    n_points = ts.size()[0]\n",
    "    n_paths = ys.size()[1]\n",
    "    \n",
    "    for i, y in enumerate(ys.permute(1,0,2)):  # iterate over paths\n",
    "        ax.plot(ts.detach().cpu().numpy(), y.detach().cpu().numpy(), lw=1, alpha=1.0, label=f'Path {i+1}' if i<10 else None)  # plot each path\n",
    "        \n",
    "    ax.set_title(f\"Sampled paths of the Ornstein-Uhlenbeck SDE - parameters : mu = {model.mu.item():.1f}, theta = {model.theta.item():.1f}, sigma = {model.sigma.item():.1f}\")\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "        \n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_ousde_samples(ts, ys, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91525a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "\n",
    "data = ys.permute(1,0,2).detach()  # shape (N_PATHS, N_POINTS, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd61e337",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Toy 1 : learning a O.U. model from the data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c702b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy_OU_SDE(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # we keep \"diagonal\" noise type\n",
    "        \n",
    "        # sde_type can be \"ito\" or \"stratonovich\"\n",
    "        # we use \"ito\" here. The available methods for computation are Euler(-Maruyama), Milstein, SRK.\n",
    "        super().__init__()\n",
    "        self.noise_type = \"diagonal\"\n",
    "        self.sde_type = \"ito\"\n",
    "        \n",
    "        # here, we want to learn the parameters theta, mu, sigma\n",
    "        self.theta = nn.Parameter(torch.tensor(1.0))  # initial guess\n",
    "        self.mu = nn.Parameter(torch.tensor(1.0))     # initial guess\n",
    "        self.sigma = nn.Parameter(torch.tensor(1.0))  # initial guess\n",
    "\n",
    "    # DRIFT FUNCTION - same signature as above\n",
    "    def f(self, t, y):\n",
    "        return self.theta * (self.mu - y)\n",
    "    \n",
    "    # DIFFUSION FUNCTION - same signature as above\n",
    "    def g(self, t, y):\n",
    "        return self.sigma**2 * torch.ones_like(y)\n",
    "    \n",
    "model_ou_1 = Toy_OU_SDE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1567ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = torchsde.sdeint(model_ou_1, y0, ts, method=\"euler\", dt=1e-3)  # shape (N_POINTS, N_PATHS, 1)\n",
    "print(f\"Computed solution samples : {preds.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7016660",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plot_ousde_samples(ts, preds, model_ou_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model\n",
    "model_ou_1 = Toy_OU_SDE().to(device)\n",
    "\n",
    "# training loop parameters\n",
    "learning_rate = 1e-2\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(model_ou_1.parameters()), \n",
    "    lr=learning_rate\n",
    ")\n",
    "\n",
    "N_EPOCHS = 1000\n",
    "\n",
    "thetas = []\n",
    "mus = []\n",
    "sigmas = []\n",
    "losses = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # compute SDE and sample N_PATHS\n",
    "    preds = torchsde.sdeint(model_ou_1, y0, ts, method=\"euler\", dt=1e-3)  # shape (N_POINTS, N_PATHS, 1)\n",
    "    # compute average L2 loss between data paths and sampled paths\n",
    "    mse = (data - preds.permute(1,0,2))**2\n",
    "    loss = mse.mean()\n",
    "    # train\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # record and report out\n",
    "    losses.append(loss.item())\n",
    "    m = model_ou_1.mu.item()\n",
    "    s = torch.sqrt(model_ou_1.sigma).item()\n",
    "    t = model_ou_1.theta.item()\n",
    "    mus.append(m)\n",
    "    sigmas.append(s)\n",
    "    thetas.append(t)\n",
    "    print(f\"Epoch {epoch+1:<3} - {N_EPOCHS:<3} - loss = {loss.item():.3e} - parametres : theta = {t:.3f}, mu = {m:.3f}, sigma = {s:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131af5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(nrows=1, ncols=4, figsize=(24,4))\n",
    "\n",
    "axs[0].plot(losses, label='losses')\n",
    "axs[1].plot(mus, label='mu')\n",
    "axs[2].plot(thetas, label='theta')\n",
    "axs[3].plot(sigmas, label='sigma')\n",
    "\n",
    "for i in range(4):\n",
    "    axs[i].set_xlabel('epoch')\n",
    "    axs[i].legend()\n",
    "    axs[i].grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d651e1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "torchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
