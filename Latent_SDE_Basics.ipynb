{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80222970",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 28px; color: black; font-weight: bold;\">\n",
    "Basics on Neural Latent SDE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38425f08",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Biblio\n",
    "</div>\n",
    "\n",
    "Neural ODEs:\n",
    "\n",
    "**Neural ODEs (https://arxiv.org/abs/1806.07366) (2019)** : introduction of the Neural ODE as the continuous-time limit of a ResNet stack. Presentation of the use of the adjoint sensitivity method. Seminal paper for Neural ODE.\n",
    "\n",
    "**Latent ODEs for Irregularly-Sampled Time Series (https://arxiv.org/abs/1907.03907) (2019)** : Evolution of the Neural ODE model towards a Neural ODE RNN model, where the approximate posterior is built with a RNN on past observations.\n",
    "\n",
    "Neural SDEs:\n",
    "\n",
    "**SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations (https://arxiv.org/abs/2502.02472 , 2025)** : good background section (#2) to explain Neural SDE. Propose a new method SDE matching, inspired by score and flow matching, vs the adjoint sensivity method. SDE matching is claimed to be more efficient to compute gradients and train latent SDEs.\n",
    "\n",
    "**Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020)** : generalization of the adjoint sensitivity method to SDEs. Combination with gradient-based stochastic variational inference for infinite-dimension VAEs.\n",
    "\n",
    "**Neural SDEs (https://www.researchgate.net/publication/333418188_Neural_Stochastic_Differential_Equations) (2019)** : link between infinitely deep residual networks and solutions to stochastic differential equations\n",
    "\n",
    "**Stable Neural SDEs in analyzing irregular time series data (https://arxiv.org/abs/2402.14989) (2025)** : points to the necessity of careful design of the drift and diffusion neural nets in latent SDEs. Introduces three latent SDEs models with performance guarantees.\n",
    "\n",
    "**Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024)** : application of neural SDEs to a biological use case (brain activity). Details the model, architecture, ELBO/loss computation. Takes into account inputs/commands in the model. \n",
    "\n",
    "General/Misc:\n",
    "\n",
    "**Efﬁcient gradient computation for dynamical models (https://www.fil.ion.ucl.ac.uk/~wpenny/publications/efficient_revised.pdf) (2014)** : summary of finite difference method, forward sensitivity method, adjoint sensitivity method, to compute gradients of a functional cost function. Applies to Neural ODEs training.\n",
    "\n",
    "**Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing (https://arxiv.org/abs/1903.10145) (2019)** : explanation of the posterior collapse/KL vanishing problem, introduces different KL annealing schedules for VAE training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be40944",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Model & Math\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4507903",
   "metadata": {},
   "source": [
    "Data : $\\mathbf{X} = (x_{t_1}, x_{t_2}, ..., x_{t_N}) \\in \\mathbb{R}^{D_x}$ - assuming all $t_i \\in [0,1]$.\n",
    "\n",
    "The latent space has dimension $D_z$. The latent continuous dynamic is $\\mathbf{Z}$ defined by:\n",
    "\\begin{align*}\n",
    "z_0^{(\\theta)} &\\sim p_{\\theta_z}(z_0) \\\\\n",
    "dz_t^{(\\theta)} &= f_{\\theta}(z_t, t)dt + \\sigma_{\\theta}(z_t,t)dB_t \n",
    "\\end{align*}\n",
    "with: \n",
    "\\begin{align}\n",
    "\\textbf{drift} \\,& f_{\\theta} : \\mathbb{R}^{D_z} \\times [0,1] \\rightarrow \\mathbb{R}^{D_z} \\\\\n",
    "\\textbf{diffusion} \\,& \\sigma_{\\theta} : \\mathbb{R}^{D_z} \\times [0,1] \\rightarrow \\mathbb{R}^{D_z \\times D_z} \\\\\n",
    "\\textbf{Brownian motion} \\,& dB_t \\in \\mathbb{R}^{D_z}\n",
    "\\end{align}\n",
    "\n",
    "The decoder is classically:\n",
    "\\begin{align}\n",
    "p_{\\theta_x}(x_{t_i} \\vert z_{t_i})\n",
    "\\end{align}\n",
    "\n",
    "The approximate posterior (encoder) is also a SDE:\n",
    "\\begin{align}\n",
    "z_0^{(\\phi)} &\\sim q_{\\phi}(z_0 \\vert \\textbf{X}) \\\\\n",
    "dz_t^{(\\phi)} &= f_{\\phi}(z_t, t, \\textbf{X})dt + \\sigma_{\\theta}(z_t,t)dB_t \n",
    "\\end{align}\n",
    "\n",
    "where:\n",
    "- the drift $f_{\\phi}(z_t, t, \\textbf{X})$ is conditionned on observations $\\textbf{X}$\n",
    "- the diffusion of the approximate posterior is shared with the diffusion of the prior : $\\sigma_{\\theta}(z_t,t)$ - this ensures the application of Girsanov theorem and a finite KL divergence between the two stochastic processes (prior and approximate posterior) (see Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024))\n",
    "- drift and diffusion neural nets do not exhibit the same convergence guarantee (Stable Neural SDEs in analyzing irregular time series data (https://arxiv.org/abs/2402.14989) (2025))\n",
    "- non-diagonal diffusion seems to be difficult to simulate and costly to approximate (Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020))\n",
    "- it seems a good practice to encode only part of the $\\textbf{X}$ in the approximate posterior : context vector (Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020)), and $t_c << t_n$ in Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093b7a86",
   "metadata": {},
   "source": [
    "Variational lower bound on the log marginal likelihood:\n",
    "\n",
    "We write:\n",
    "\\begin{align}\n",
    "p(x_{t_1:t_N}) &= \\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{p(z_{t_1:t_N} \\vert x_{t_1:t_N})}\n",
    "\\end{align}\n",
    "And:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}\\frac{q_{\\phi}(z\\vert X)}{p(z_{t_1:t_N} \\vert x_{t_1:t_N})}} dz\n",
    "\\end{align}\n",
    "where $q_{\\phi}(z \\vert X)$ is formally is posterior distribution over **functions** $z : \\mathbb{R} \\rightarrow \\mathbb{R}^{D_z}$.\n",
    "Then:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}} dz + \\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N} \\vert x_{t_1:t_N}))\n",
    "\\end{align}\n",
    "where we -audaciously- consider $p(z_{t_1:t_N} \\vert x_{t_1:t_N})$ as a dsitribution over functions $z$ taking values $z_{t_1:t_N}$ at times $t_1:t_N$ so the $\\mathbb{KL}$ actually means something.\n",
    "Still on the same path:\n",
    "\\begin{align}\n",
    "\\log{p(x_{t_1:t_N})} &\\geq \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N}, z_{t_1:t_N})}{q_{\\phi}(z\\vert X)}} dz \\\\\n",
    "&= \\int q_{\\phi}(z \\vert X) \\log{\\frac{p(x_{t_1:t_N} \\vert z_{t_1:t_N})}{q_{\\phi}(z\\vert X)} p(z_{t_1:t_N})} dz \\\\\n",
    "&= \\mathbb{E}_{q_{\\phi}(z \\vert X)} \\log{p(x_{t_1:t_N} \\vert z_{t_1:t_N})} - \\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N})) \\\\\n",
    "\\end{align}\n",
    "We write -still audaciously-\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(q_{\\phi}(z\\vert X) \\vert\\vert p(z_{t_1:t_N})) &= \\mathbb{KL}(q_{\\phi}(z_0\\vert X) \\vert\\vert p_{\\theta_z}(z_0)) + \\mathbb{KL}(q_{\\phi}(z_{>0}\\vert X) \\vert\\vert p_{\\theta_z}(z_{>0}))\n",
    "\\end{align}\n",
    "where the first $\\mathbb{KL}$ on the r.h.s is a classic between two probability distributions over a random variable, and the second is derived from the Girsanov's theorem as:\n",
    "\\begin{align}\n",
    "\\mathbb{KL}(q_{\\phi}(z_{>0}\\vert X) \\vert\\vert p_{\\theta_z}(z_{>0})) &= \\frac{1}{2} \\mathbb{E}_{q_{\\phi}(z_{>0}\\vert X)} \\left( \\int_{0}^{T} \\vert \\Delta(t) \\vert^2 dt \\right) \\\\\n",
    "\\Delta(t) &= \\sigma_{\\theta}^{-1}(z_t,t) (f_{\\phi}(z_t, t, \\textbf{X}) - f_{\\theta}(z_t, t))\n",
    "\\end{align}\n",
    "\n",
    "Finally:\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi, \\textbf{X}) &= \\mathbb{E}_{q_{\\phi}(z \\vert X)} \\log{p(x_{t_1:t_N} \\vert z_{t_1:t_N})} - \\mathbb{KL}(q_{\\phi}(z_0\\vert X) \\vert\\vert p_{\\theta_z}(z_0)) - \\frac{1}{2} \\mathbb{E}_{q_{\\phi}(z_{>0}\\vert X)} \\left( \\int_{0}^{T} \\vert \\Delta(t) \\vert^2 dt \\right)\n",
    "\\end{align}\n",
    "\n",
    "During training:\n",
    "- the integral is approximated via numerical integration\n",
    "- expectations are estimated with MC sampling\n",
    "- NB : sampling is actually : sampling $z_0 \\sim q_{\\phi}(z_0 \\vert \\textbf{X})$ and sampling a function $z$ by sampling a Brownian motion path $B_t$ and computing the whole realization path $z_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d885ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Code\n",
    "</div>\n",
    "\n",
    "https://github.com/google-research/torchsde\n",
    "\n",
    "[1] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud. \"Scalable Gradients for Stochastic Differential Equations\". International Conference on Artificial Intelligence and Statistics. 2020. [arXiv]\n",
    "\n",
    "[2] Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, Terry Lyons. \"Neural SDEs as Infinite-Dimensional GANs\". International Conference on Machine Learning 2021. [arXiv]\n",
    "\n",
    "[3] Patrick Kidger, James Foster, Xuechen Li, Terry Lyons. \"Efficient and Accurate Gradients for Neural SDEs\". 2021. [arXiv]\n",
    "\n",
    "[4] Patrick Kidger, James Morrill, James Foster, Terry Lyons, \"Neural Controlled Differential Equations for Irregular Time Series\". Neural Information Processing Systems 2020. [arXiv]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31edd02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0233e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082258c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7c7cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c3e128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timeit\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "216f244b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9acec511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "GPU Name: NVIDIA GeForce RTX 4090\n",
      "Total GPU Memory: 23.6 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9108d4f2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "The adjoint method\n",
    "</div>\n",
    "\n",
    "- Solve the $\\textbf{initial value problem}$ with an ODE solver:\n",
    "                $\\begin{align}\n",
    "                    \\frac{dx}{dt} &= f_{\\theta}(x,t) \\\\\n",
    "                    x(t=0) &= x_0\n",
    "                \\end{align}$\n",
    "- Solve the $\\textbf{adjoint/terminal value problem}$ with an ODE solver:\n",
    "                $\\begin{align}\n",
    "                    \\frac{d \\lambda}{dt} + \\left( \\frac{\\partial f}{\\partial x}\\right)^{T} \\lambda(t) + \\left( \\frac{\\partial g}{\\partial x}\\right)^{T} &= 0 \\\\\n",
    "                    \\lambda(T) &= 0\n",
    "                \\end{align}$\n",
    "- Compute the gradient:\n",
    "                $\\begin{align}\n",
    "                    \\frac{dJ}{d \\theta} &= \\int_{0}^{T} \\left( \\frac{\\partial g}{\\partial \\theta} + \\lambda^{T}(t) \\frac{\\partial f}{\\partial \\theta} \\right) dt\n",
    "                        + \\lambda(0) \\frac{df\\vert_0}{d\\theta}\n",
    "                \\end{align}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "torchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
