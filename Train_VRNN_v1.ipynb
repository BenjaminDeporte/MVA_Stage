{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "273af21f",
   "metadata": {},
   "source": [
    "VAEs dynamiques : https://arxiv.org/abs/2008.12595\n",
    "\n",
    "# Training a Variational RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f61dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a4f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.vrnn_lib import seed_everything, VRNN\n",
    "from libs.vrnn_lib import loss_function, train\n",
    "from libs.vrnn_lib import BetaLinearScheduler, BetaThresholdScheduler\n",
    "from libs.vrnn_lib import plot_losses, sample_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eba1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484759d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49afe4e",
   "metadata": {},
   "source": [
    "# Modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59c3e4",
   "metadata": {},
   "source": [
    "### Variational RNN :\n",
    "\n",
    "The full factorization of $p(z_{1:T}, x_{1:T})$ is used:\n",
    "\n",
    "\\begin{align}\n",
    "p(z_{1:T}, x_{1:T}) &= \\prod_{t=1}^T p(z_t, x_t \\vert z_{1:t-1}, x_{1:t-1}) \\\\\n",
    "&= \\prod_{t=1}^T p_{\\theta_x}(x_t \\vert x_{1:t-1}, z_{1:t}) p_{\\theta_z}(z_t \\vert z_{1:t-1}, x_{1:t-1}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Where the two distributions $p_{\\theta_z}$ and $p_{\\theta_x}$ are Gaussians :\n",
    "\n",
    "\\begin{align}\n",
    "\n",
    "p_{\\theta_x}(x_t \\vert x_{1:t-1}, z_{1:t}) &= \\mathcal{N}(x_t \\vert \\mu_{\\theta_x}(\\overrightarrow{g}_{t-1}, h_{t}), \\text{diag}(\\sigma_{\\theta_x}^{2}(\\overrightarrow{g}_{t-1}, h_{t}))) \\\\\n",
    "d_x(\\overrightarrow{g}_{t-1}, h_{t}) &= [ \\mu_{\\theta_x}(\\overrightarrow{g}_{t-1}, h_{t}), \\sigma_{\\theta_x}(\\overrightarrow{g}_{t-1}, h_{t}) ] \\\\\n",
    "\n",
    "p_{\\theta_z}(z_t \\vert z_{1:t-1}, x_{1:t-1}) &= \\mathcal{N}(z_t \\vert \\mu_{\\theta_z}(\\overrightarrow{g}_{t-1}, h_{t-1}), \\text{diag}(\\sigma_{\\theta_z}^{2}(\\overrightarrow{g}_{t-1}, h_{t-1}))) \\\\\n",
    "d_z(\\overrightarrow{g}_{t-1}, h_{t-1}) &= [ \\mu_{\\theta_z}(\\overrightarrow{g}_{t-1}, h_{t-1}), \\sigma_{\\theta_z}(\\overrightarrow{g}_{t-1}, h_{t-1}) ] \\\\\n",
    "\n",
    "\\end{align}\n",
    "\n",
    "où $d_x, d_z$ sont des MLP,\n",
    "\n",
    "et où $h_t = \\text{LSTM}(z_t, h_{t-1})$ est un forward LSTM qui encode $z_{1:t}$\n",
    "\n",
    "et $\\overrightarrow{g}_t = \\text{LSTM}(x_t, \\overrightarrow{g}_{t-1})$ est un forward LSTM qui encode $x_{1:t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118ab0a",
   "metadata": {},
   "source": [
    "### Modèle inférence\n",
    "\n",
    "Le \"true posterior\" s'écrit :\n",
    "\\begin{align}\n",
    "p_{\\theta}(z_{1:T} \\vert x_{1:T}) &= \\prod_{t=1}^T p_{\\theta} (z_t \\vert z_{1:t-1}, x_{1:T} ) \\\\\n",
    "\\end{align}\n",
    "\n",
    "où la première écriture est la stricte application de la chain rule.\n",
    "\n",
    "On choisit comme approximation du posterior (=encodeur) une formulation calquée sur le vrai posterior :\n",
    "\n",
    "\\begin{align}\n",
    "q_{\\phi}(z_{1:T} \\vert x_{1:T}) &= \\prod_{t=1}^T q_{\\phi} (z_{t} \\vert z_{1:t-1}, x_{1:T}) \\\\\n",
    "\\end{align}\n",
    "\n",
    "Et:\n",
    "\\begin{align}\n",
    "q_{\\phi} (z_{t} \\vert z_{1:t-1}, x_{1:T}) &= \\mathcal{N}(z_t \\vert \\mu_{\\phi}(h_{t-1}, \\overrightarrow{g}_{t-1}, \\overleftarrow{g}_{t}), \\text{diag} (\\sigma_{\\phi}^2(h_{t-1}, \\overrightarrow{g}_{t-1}, \\overleftarrow{g}_{t}))) \\\\\n",
    "d_{\\phi}(h_{t-1}, \\overrightarrow{g}_{t-1}, \\overleftarrow{g}_{t}) &= [ \\mu_{\\phi}(h_{t-1}, \\overrightarrow{g}_{t-1}, \\overleftarrow{g}_{t}), \\sigma_{\\phi}(h_{t-1}, \\overrightarrow{g}_{t-1}, \\overleftarrow{g}_{t}) ]\n",
    "\\end{align}\n",
    "\n",
    "où $d_{\\phi}$ est un MLP, et\n",
    "\n",
    "$\\overleftarrow{g}_t = \\text{LSTM}(x_t, \\overleftarrow{g}_{t-1})$ est un backward LSTM qui encode $x_{t:T}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34ff007",
   "metadata": {},
   "source": [
    "# Implémentation de l'inférence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945b1b3",
   "metadata": {},
   "source": [
    "Il y a ici quelques choix d'implémentations différents de ceux du papier originel:\n",
    "\n",
    "- **trois** LSTMs distincts pour encoder $z_{1:t}$, $x_{1:t}$ et $x_{t:T}$\n",
    "\n",
    "- pas d'extracteurs de features sur $x$ et $z$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d1c6e0",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f957c3",
   "metadata": {},
   "source": [
    "Le modèle s'entraîne en maximisant un ELBO, dont la formulation générique se simplifie dans le cas du DKF en :\n",
    "\n",
    "\\begin{align}\n",
    "\\mathcal{L}(\\theta, \\phi; X) &= \\sum_{t=1}^T \\mathbb{E}_{q_\\phi(z_t \\vert x_{1:T})} \\log(p_{\\theta_x}(x_t \\vert z_t)) -\n",
    "\\sum_{t=1}^T \\mathbb{E}_{q_\\phi(z_{t-1} \\vert x_{1:T})} \\text{D}_{\\text{KL}} \\left[ q_\\phi(z_t \\vert z_{t-1}, x_{t:T}) \\vert\\vert \n",
    "p_{\\theta_z}(z_t \\vert z_{t-1}) \\right]\n",
    "\\end{align}\n",
    "\n",
    "Les deux termes s'explicitent de la façon suivante (avec $D$ dimension de l'espace des observations) :\n",
    "\n",
    "\\begin{align}\n",
    "p_{\\theta_x}(x_t \\vert z_t) &= \\mathcal{N}(x_t \\vert \\mu_{\\theta_x}(z_t), \\text{diag}(\\sigma_{\\theta_x}^2(z_t))) \\\\\n",
    "\\log{p_{\\theta_x}(x_t \\vert z_t)} &= -\\frac{D}{2} \\log{2\\pi} - \\frac{1}{2}\\log{\\vert \\text{diag}(\\sigma_{\\theta_x}^2(z_t)) \\vert} - \n",
    "\\frac{1}{2} \\left[ (x_t - \\mu_{\\theta_x}(z_t))^T (\\text{diag}(\\sigma_{\\theta_x}^2(z_t)))^{-1} (x_t - \\mu_{\\theta_x}(z_t)) \\right] \\\\\n",
    "&= - \\frac{1}{2} \\left( \\sum_{i=1}^D \\log{\\sigma_{\\theta_x}^2(z_t)}\\vert_{i} + (x_t - \\mu_{\\theta_x}(z_t))^T \\text{diag} \\frac{1}{\\sigma_{\\theta_x}^2(z_t)} (x_t - \\mu_{\\theta_x}(z_t)) + D \\log{2\\pi} \\right)\n",
    "\\end{align}\n",
    "\n",
    "Et la KL entre les deux Gaussiennes:\n",
    "\n",
    "\\begin{align}\n",
    "q_\\phi(z_t \\vert z_{t-1}, x_{t:T}) &= \\mathcal{N}(z_t \\vert \\mu_{\\phi}(g_t), \\text{diag}(\\sigma_\\phi^2(g_t))) \\\\\n",
    "p_{\\theta_z}(z_t \\vert z_{t-1}) &= \\mathcal{N}(z_t \\vert \\mu_{\\theta_z}(z_{t-1}), \\text{diag}(\\sigma_{\\theta_z}^{2}(z_{t-1}))) \\\\\n",
    "\\end{align}\n",
    "\n",
    "a une close form (avec $Z$ dimension de l'espace latent):\n",
    "\n",
    "\\begin{align}\n",
    "\\text{D}_{\\text{KL}}(q_\\phi \\vert\\vert p_{\\theta_z}) &= \\frac{1}{2} \\left[ \\text{Tr}(\\text{diag}(\\sigma_{\\theta_z}^{2})^{-1} \\text{diag}(\\sigma_\\phi^2) ) + (\\mu_{\\theta_z} - \\mu_\\phi)^T (\\text{diag}(\\sigma_{\\theta_z}^{2})^{-1}) (\\mu_{\\theta_z} - \\mu_\\phi) +\n",
    "\\log{\\frac{\\vert \\text{diag}(\\sigma_{\\theta_z}^{2})\\vert}{\\vert \\text{diag}(\\sigma_\\phi^2) \\vert} } \\right] \\\\\n",
    "&= \\frac{1}{2}\\left[ \\sum_{i=1}^Z \\log{\\sigma_{\\theta_z}^{2}}\\vert_i - \\sum_{i=1}^Z \\log{\\sigma_{\\phi}^{2}}\\vert_i +\n",
    " (\\mu_{\\theta_z} - \\mu_\\phi)^T \\text{diag}(\\frac{1}{\\sigma_{\\theta_z}^{2}}) (\\mu_{\\theta_z} - \\mu_\\phi) + \\sum_{i=1}^Z \\frac{\\sigma_{\\phi}^{2}\\vert_i} {\\sigma_{\\theta_z}^{2}\\vert_i} - Z \n",
    "\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a4e53",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfdb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1 # Dimension of the observation space\n",
    "Z_DIM = 2 # Dimension of the latent space\n",
    "RNN_X_H_DIM = 4 # Dimension of the hidden state of the bidirectional LSTM network for observations\n",
    "RNN_Z_H_DIM = 8 # Dimension of the hidden state of the bidirectional LSTM network for latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca53cc",
   "metadata": {},
   "source": [
    "# Toy Case : Data Generation for Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5ce750",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 25\n",
    "n_ahead = 5\n",
    "n_series = 100\n",
    "\n",
    "def generate_time_series(batch_size, n_steps, noise=0.05):\n",
    "    \"\"\"Utility function to generate time series data.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): number of time series to generate (btach size)\n",
    "        n_steps (_type_): length of each time series\n",
    "    \"\"\"\n",
    "    \n",
    "    f1,f2,o1,o2 = np.random.rand(4, batch_size, 1)  # return 4 values for each time series\n",
    "    time = np.linspace(0, 1, n_steps)  # time vector\n",
    "    \n",
    "    series = 0.4 * np.sin((time - o1) * (f1 * 5 + 10)) # first sine wave\n",
    "    series += 0.2 * np.sin((time - o2) * (f2 * 20 + 20)) # second sine wave\n",
    "    series += noise * (np.random.randn(batch_size, n_steps) - 0.5)  # add noise\n",
    "    \n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64391527",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = generate_time_series(n_series, n_steps+n_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cdb395",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 3\n",
    "fig, axs = plt.subplots(N, 1, figsize=(16, 3 * N))\n",
    "for i in range(N):\n",
    "    axs[i].plot(s[i], color='blue', marker=\"x\", linewidth=1)\n",
    "    axs[i].set_title(f\"Time series {i+1}\")\n",
    "    axs[i].set_xlabel(\"Time\")\n",
    "    axs[i].set_ylabel(\"Value\")\n",
    "    axs[i].grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5a0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = int(0.8 * n_series)\n",
    "\n",
    "X_train, y_train = s[:cutoff,:n_steps], s[:cutoff,n_steps:]\n",
    "X_valid, y_valid = s[cutoff:,:n_steps], s[cutoff:,n_steps:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_valid shape: {X_valid.shape}\")\n",
    "print(f\"y_valid shape: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c067913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# form datasets, dataloaders, etc\n",
    "\n",
    "BATCH_SIZE = 16  # 8192 ok sur RTX3080 et 150 time steps\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X).to(device)\n",
    "        self.y = torch.tensor(y).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "test_dataset = TimeSeriesDataset(X_valid, y_valid)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dea0e76",
   "metadata": {},
   "source": [
    "# Baseline : RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff5f4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelLookAhead(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, n_ahead=n_ahead, num_layers=1, batch_first=True, device=device, dtype=dtype):\n",
    "        \"\"\"Constructor for RNN.\n",
    "\n",
    "        Args:\n",
    "            input_dim (_type_): dimensionality of the input\n",
    "            hidden_dim (_type_): dimensionality of the hidden state\n",
    "            n_ahead (_type_, optional): number of time steps to predict. Defaults to N_AHEAD.\n",
    "            output_dim (_type_, optional): dimensionality of the output.\n",
    "            num_layers (int, optional): number of recurrent layers. Defaults to 1.\n",
    "            batch_first (bool, optional): whether batch dim is first or not. Defaults to True.\n",
    "                1. batch_first=True: (batch, seq, feature_dimension)\n",
    "                2. batch_first=False: (seq, batch, feature_dimension)\n",
    "            bidirectional (bool, optional): if True, becomes a bidriectional RNN. Defaults to False.\n",
    "                1. bidirectional=True: num_directions=2, (batch, seq, hidden_dim * 2)\n",
    "                2. bidirectional=False: num_directions=1, (batch, seq, hidden_dim)\n",
    "            device (_type_, optional): _description_. Defaults to device.\n",
    "            dtype (_type_, optional): _description_. Defaults to dtype.\n",
    "        \"\"\"\n",
    "        super(RNNModelLookAhead, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bidirectional = False\n",
    "        self.n_ahead = n_ahead\n",
    "        \n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=batch_first,\n",
    "            bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, n_ahead*output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # first, initialize the hidden state\n",
    "        h0 = torch.zeros((self.num_layers, x.size(0), self.hidden_dim), requires_grad=True).to(device)\n",
    "        # INPUT : x : (batch, sequence_length, input_feature_dimension)\n",
    "        x, _ = self.rnn(x, h0) \n",
    "        # OUTPUT: \n",
    "        # - output : (batch, sequence_length, hidden_dimension * num_directions)\n",
    "        # - h_n : (num_layers * num_directions, batch, hidden_dimension) (hidden state for last time step)\n",
    "        x = self.fc(x[:, -1, :])  # take the last time step\n",
    "        x = x.view(-1, self.n_ahead, self.output_dim)\n",
    "        # OUTPUT: x : (batch, output_dimension)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949607b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNNModelLookAhead(\n",
    "    input_dim=1,\n",
    "    output_dim=1,\n",
    "    n_ahead=n_ahead,\n",
    "    hidden_dim=128,\n",
    "    num_layers=4,\n",
    "    batch_first=True,\n",
    "    device=device,\n",
    "    dtype=dtype\n",
    ").to(device)\n",
    "\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dimensions\n",
    "\n",
    "x = torch.randn(BATCH_SIZE, 50, 1).to(device)\n",
    "y = rnn(x)\n",
    "print(f\"input shape: {x.shape}\")\n",
    "print(f\"output shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314e7eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6818ffc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e892aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "print(f\"Start training RNN model for {num_epochs} epochs\")\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    # loop on training data\n",
    "    rnn.train()\n",
    "    optimizer.zero_grad()\n",
    "    ### loop on training data\n",
    "    epoch_loss = 0\n",
    "    for input, target in train_loader:\n",
    "        input = input.to(device).unsqueeze(-1)  # add a feature dimension\n",
    "        # print(f\"input has shape {input.shape}\")\n",
    "        target = target.to(device).view(-1, n_ahead, 1)\n",
    "        # print(f\"target has shape {target.shape}\")\n",
    "        output = rnn(input)\n",
    "        # print(F\"output has shape {output.shape}\")\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    epoch_loss /= len(train_loader) \n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # test step\n",
    "    rnn.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input = input.to(device).unsqueeze(-1)  # add a feature dimension\n",
    "            target = target.to(device).view(-1, n_ahead, 1)\n",
    "            output = rnn(input)\n",
    "            loss = criterion(output, target)\n",
    "            epoch_loss += loss.item()\n",
    "    epoch_loss /= len(test_loader)\n",
    "    valid_losses.append(epoch_loss)\n",
    "    \n",
    "    # report out\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(f\"epoch {i+1:>4}/{num_epochs}, training loss = {train_losses[-1]:.4e}, validation loss = {valid_losses[-1]:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f32d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(16, 3))\n",
    "ax.plot(train_losses, label=\"train\")\n",
    "ax.plot(valid_losses, label=\"valid\")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_xticks(np.arange(0, num_epochs+1, 10))\n",
    "ax.set_xticklabels(np.arange(0, num_epochs+1, 10))\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Training and Validation Loss\")\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnn(torch.tensor(X_valid).to(device).unsqueeze(-1))\n",
    "y_target = torch.tensor(y_valid).to(device).unsqueeze(-1)\n",
    "\n",
    "# print(f\"y_target shape: {y_target.shape}\")\n",
    "# print(f\"y_pred shape: {y_pred.shape}\")\n",
    "\n",
    "print(f\"Loss finale = {criterion(y_pred, y_target):.4e}\")\n",
    "\n",
    "y_pred = y_pred.cpu().detach().numpy()\n",
    "y_target = y_target.cpu().detach().numpy()\n",
    "\n",
    "# print(f\"\\n{np.mean(np.sqrt((y_target - y_pred) ** 2)):.4f} RMSE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc467d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "fig, ax  = plt.subplots(N, 1, figsize=(14, 2 * N))\n",
    "x_shift = X_valid.shape[-1]\n",
    "\n",
    "for i in range(N):\n",
    "    input = torch.tensor(X_valid[i], device=device).unsqueeze(0).unsqueeze(-1)\n",
    "    # input = input.permute(1, 0, 2)  # permute to (seq_len, batch_size, input_dim)\n",
    "    # print(f\"input has shape {input.shape}\")\n",
    "    target = torch.tensor(y_valid[i], device=device).view(-1, n_ahead, 1)\n",
    "    # target = target.permute(1, 0, 2)  # permute to (seq_len, batch_size, output_dim)\n",
    "    # print(f\"target has shape {target.shape}\")\n",
    "    output = rnn(input)\n",
    "    output = output.cpu().detach().numpy()\n",
    "    # print(f\"output has shape {output.shape}\")\n",
    "    target = target.cpu().detach().numpy()\n",
    "    \n",
    "    ax[i].plot(input.squeeze().cpu().detach().numpy(), color='blue', marker=\".\", linewidth=1, label=\"input\")\n",
    "    ax[i].plot(np.arange(len(target.squeeze()))+x_shift, target.squeeze(), color='red', marker=\"o\", linewidth=1, label=\"ground truth\")\n",
    "    ax[i].plot(np.arange(len(target.squeeze()))+x_shift, output.squeeze(), color='green', marker=\"*\", linewidth=1, label=\"prediction\")\n",
    "    ax[i].set_title(f\"Time series {i+1}\")\n",
    "    ax[i].set_xlabel(\"Time\")\n",
    "    ax[i].set_ylabel(\"Value\")\n",
    "    ax[i].legend()\n",
    "    ax[i].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd54c5d6",
   "metadata": {},
   "source": [
    "# Training VRNN\n",
    "\n",
    "https://www.youtube.com/watch?v=rz76gYgxySo&list=WL&index=1&t=1618s&ab_channel=SimonLeglaive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb74eb9",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"/home/benjamin.deporte/MVA/MVA_Stage/vrnn3.jpg\" width=\"1000\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665567dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_DIM = 1 # Dimension of the observation space\n",
    "Z_DIM = 8 # Dimension of the latent space\n",
    "RNN_X_H_DIM = 16 # Dimension of the hidden state of the bidirectional LSTM network for observations\n",
    "RNN_Z_H_DIM = 16 # Dimension of the hidden state of the bidirectional LSTM network for latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b376d0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vrnn = VRNN(\n",
    "    input_dim = X_DIM,\n",
    "    latent_dim = Z_DIM,\n",
    "    rnn_x_hidden_dim = RNN_X_H_DIM,\n",
    "    rnn_z_hidden_dim = RNN_Z_H_DIM,\n",
    "    device=device\n",
    ").to(device)\n",
    "\n",
    "print(vrnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ab5eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(vrnn.parameters(), lr=1e-3)\n",
    "loss_fn = loss_function\n",
    "\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23628e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "n_displays = 100\n",
    "display_frequency = int(num_epochs / n_displays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f9c604",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = 1.0\n",
    "\n",
    "# beta_scheduler = BetaLinearScheduler(\n",
    "#     beta_start=1e-3,\n",
    "#     beta_end=1.0,\n",
    "#     epoch_start=150,\n",
    "#     epoch_end=200,\n",
    "#     num_epochs=None\n",
    "# )\n",
    "\n",
    "# beta_scheduler = BetaThresholdScheduler(\n",
    "#     rec_loss_threshold=0.0,\n",
    "#     beta_start=1e-3,\n",
    "#     beta_end=1.0,\n",
    "#     num_epochs=100\n",
    "# )\n",
    "\n",
    "# print(beta_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ad136f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_losses, kl_losses, epoch_losses, val_rec_losses, val_kl_losses, val_epoch_losses, betas = train(\n",
    "    vrnn, \n",
    "    optimizer, \n",
    "    loss_fn, \n",
    "    num_epochs=num_epochs, \n",
    "    train_loader=train_loader, \n",
    "    test_loader=test_loader, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    device=device, \n",
    "    beta=beta, \n",
    "    beta_scheduler=None, \n",
    "    display_frequency=display_frequency, \n",
    "    K=K\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(rec_losses, kl_losses, epoch_losses, val_rec_losses, val_kl_losses, val_epoch_losses, betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30200725",
   "metadata": {},
   "source": [
    "# Predictions / Générations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_ahead = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a59a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_predictions(N_SAMPLES=30, model=vrnn, X_valid=X_valid, y_valid=y_valid, n_steps=n_steps, n_ahead=n_ahead, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
