% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.3 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global/global}
    \entry{bishop_pattern_2016}{book}{}{}
      \name{author}{1}{}{%
        {{hash=9dfd0135a5e80aa6d81cea2c10fb7f73}{%
           family={Bishop},
           familyi={B\bibinitperiod},
           given={Christopher\bibnamedelima M.},
           giveni={C\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \list{location}{1}{%
        {New York, NY}%
      }
      \list{publisher}{1}{%
        {Springer New York}%
      }
      \strng{namehash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{fullhash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{fullhashraw}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{bibnamehash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{authorbibnamehash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{authornamehash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{authorfullhash}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \strng{authorfullhashraw}{9dfd0135a5e80aa6d81cea2c10fb7f73}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The field of pattern recognition has undergone substantial development over the years. This book reflects these developments while providing a grounding in the basic concepts of pattern recognition and machine learning. It is aimed at advanced undergraduates or first year PhD students, as well as researchers and practitioners}
      \field{edition}{Softcover reprint of the original 1st edition 2006 (corrected at 8th printing 2009)}
      \field{isbn}{9781493938438}
      \field{series}{Information science and statistics}
      \field{title}{Pattern {Recognition} and {Machine} {Learning}}
      \field{year}{2016}
    \endentry
    \entry{fortuin_gp-vae:_2020}{misc}{}{}
      \name{author}{4}{}{%
        {{hash=47bbd0e06f9a2791df9839ddb78957f6}{%
           family={Fortuin},
           familyi={F\bibinitperiod},
           given={Vincent},
           giveni={V\bibinitperiod}}}%
        {{hash=28980623184fdb1179afe1edbaee455a}{%
           family={Baranchuk},
           familyi={B\bibinitperiod},
           given={Dmitry},
           giveni={D\bibinitperiod}}}%
        {{hash=ca596a0a9d87ca9f7999db4d71c7e577}{%
           family={Rätsch},
           familyi={R\bibinitperiod},
           given={Gunnar},
           giveni={G\bibinitperiod}}}%
        {{hash=01767107e2c06a4879e3fff5cb6116b1}{%
           family={Mandt},
           familyi={M\bibinitperiod},
           given={Stephan},
           giveni={S\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{96639821095b5475dcdceff31d77658c}
      \strng{fullhash}{dea7428c602ae28893a4703528ef8ff9}
      \strng{fullhashraw}{dea7428c602ae28893a4703528ef8ff9}
      \strng{bibnamehash}{96639821095b5475dcdceff31d77658c}
      \strng{authorbibnamehash}{96639821095b5475dcdceff31d77658c}
      \strng{authornamehash}{96639821095b5475dcdceff31d77658c}
      \strng{authorfullhash}{dea7428c602ae28893a4703528ef8ff9}
      \strng{authorfullhashraw}{dea7428c602ae28893a4703528ef8ff9}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, naive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.}
      \field{month}{2}
      \field{note}{arXiv:1907.04155}
      \field{shorttitle}{{GP}-{VAE}}
      \field{title}{{GP}-{VAE}: {Deep} {Probabilistic} {Time} {Series} {Imputation}}
      \field{urlday}{7}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{year}{2020}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.1907.04155
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1907.04155
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1907.04155
      \endverb
      \keyw{Statistics - Machine Learning,Computer Science - Machine Learning}
    \endentry
    \entry{noauthor_gaussian_nodate}{misc}{}{}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labeltitlesource}{title}
      \field{title}{Gaussian {Processes} for {Machine} {Learning}: {Book} webpage}
      \field{urlday}{7}
      \field{urlmonth}{6}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://gaussianprocess.org/gpml/
      \endverb
      \verb{url}
      \verb https://gaussianprocess.org/gpml/
      \endverb
    \endentry
    \entry{girin_dynamical_2022}{misc}{}{}
      \name{author}{6}{}{%
        {{hash=90d860edd4de0d85574a26aecb7d08ea}{%
           family={Girin},
           familyi={G\bibinitperiod},
           given={Laurent},
           giveni={L\bibinitperiod}}}%
        {{hash=63a4c46417bc2baaceb1d8e6a677316f}{%
           family={Leglaive},
           familyi={L\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=46e30335817ff3dd17f8c53b12a687e9}{%
           family={Bie},
           familyi={B\bibinitperiod},
           given={Xiaoyu},
           giveni={X\bibinitperiod}}}%
        {{hash=91694a4ce074b15cb4cc32df6d2eb46b}{%
           family={Diard},
           familyi={D\bibinitperiod},
           given={Julien},
           giveni={J\bibinitperiod}}}%
        {{hash=0829ff44458126cbdbcba2077124b43d}{%
           family={Hueber},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=dc307510b5416849a7b4bf16806b21ed}{%
           family={Alameda-Pineda},
           familyi={A\bibinithyphendelim P\bibinitperiod},
           given={Xavier},
           giveni={X\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{87248adcd7ba6de2d1a076fd3110b98e}
      \strng{fullhash}{44acacb6fe5a3e8a4df930c47853405d}
      \strng{fullhashraw}{44acacb6fe5a3e8a4df930c47853405d}
      \strng{bibnamehash}{87248adcd7ba6de2d1a076fd3110b98e}
      \strng{authorbibnamehash}{87248adcd7ba6de2d1a076fd3110b98e}
      \strng{authornamehash}{87248adcd7ba6de2d1a076fd3110b98e}
      \strng{authorfullhash}{44acacb6fe5a3e8a4df930c47853405d}
      \strng{authorfullhashraw}{44acacb6fe5a3e8a4df930c47853405d}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Variational autoencoders (VAEs) are powerful deep generative models widely used to represent high-dimensional complex data through a low-dimensional latent space learned in an unsupervised manner. In the original VAE model, the input data vectors are processed independently. Recently, a series of papers have presented different extensions of the VAE to process sequential data, which model not only the latent space but also the temporal dependencies within a sequence of data vectors and corresponding latent vectors, relying on recurrent neural networks or state-space models. In this paper, we perform a literature review of these models. We introduce and discuss a general class of models, called dynamical variational autoencoders (DVAEs), which encompasses a large subset of these temporal VAE extensions. Then, we present in detail seven recently proposed DVAE models, with an aim to homogenize the notations and presentation lines, as well as to relate these models with existing classical temporal models. We have reimplemented those seven DVAE models and present the results of an experimental benchmark conducted on the speech analysis-resynthesis task (the PyTorch code is made publicly available). The paper concludes with a discussion on important issues concerning the DVAE class of models and future research guidelines.}
      \field{month}{7}
      \field{note}{arXiv:2008.12595}
      \field{shorttitle}{Dynamical {Variational} {Autoencoders}}
      \field{title}{Dynamical {Variational} {Autoencoders}: {A} {Comprehensive} {Review}}
      \field{urlday}{19}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{year}{2022}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2008.12595
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2008.12595
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2008.12595
      \endverb
      \keyw{Computer Science - Machine Learning,Statistics - Machine Learning}
    \endentry
    \entry{kingma_introduction_2019}{article}{}{}
      \name{author}{2}{}{%
        {{hash=b6fbd171848aad4edf3925543f1f1522}{%
           family={Kingma},
           familyi={K\bibinitperiod},
           given={Diederik\bibnamedelima P.},
           giveni={D\bibinitperiod\bibinitdelim P\bibinitperiod}}}%
        {{hash=53d2880ad8047b61cdae2c6b2803e763}{%
           family={Welling},
           familyi={W\bibinitperiod},
           given={Max},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{fullhash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{fullhashraw}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{bibnamehash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{authorbibnamehash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{authornamehash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{authorfullhash}{aabdd5db7a1ed298b1dfb6824d032c66}
      \strng{authorfullhashraw}{aabdd5db7a1ed298b1dfb6824d032c66}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.}
      \field{title}{An {Introduction} to {Variational} {Autoencoders}}
      \field{urlday}{21}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/ARXIV.1906.02691
      \endverb
      \verb{urlraw}
      \verb https://arxiv.org/abs/1906.02691
      \endverb
      \verb{url}
      \verb https://arxiv.org/abs/1906.02691
      \endverb
      \keyw{Machine Learning (cs.LG),Machine Learning (stat.ML),FOS: Computer and information sciences,FOS: Computer and information sciences}
    \endentry
    \entry{ProbabilisticGraphicalModels}{misc}{}{}
      \name{author}{1}{}{%
        {{hash=a184f9d88214b4981730114ccf21ad78}{%
           family={Koller},
           familyi={K\bibinitperiod},
           given={Friedman},
           giveni={F\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {en-US}%
      }
      \strng{namehash}{a184f9d88214b4981730114ccf21ad78}
      \strng{fullhash}{a184f9d88214b4981730114ccf21ad78}
      \strng{fullhashraw}{a184f9d88214b4981730114ccf21ad78}
      \strng{bibnamehash}{a184f9d88214b4981730114ccf21ad78}
      \strng{authorbibnamehash}{a184f9d88214b4981730114ccf21ad78}
      \strng{authornamehash}{a184f9d88214b4981730114ccf21ad78}
      \strng{authorfullhash}{a184f9d88214b4981730114ccf21ad78}
      \strng{authorfullhashraw}{a184f9d88214b4981730114ccf21ad78}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most tasks require a person or an automated system to reason—to reach conclusions based on available information. The framework of probabilistic graphical ...}
      \field{journaltitle}{MIT Press}
      \field{title}{Probabilistic {Graphical} {Models}}
      \field{urlday}{21}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/
      \endverb
      \verb{url}
      \verb https://mitpress.mit.edu/9780262013192/probabilistic-graphical-models/
      \endverb
    \endentry
    \entry{mouvement-brownien-calcul-ito}{misc}{}{}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labeltitlesource}{title}
      \field{abstract}{Le mouvement désordonné de particules de pollen en suspension dans un liquide en équilibre fut observé et rigoureusement rapporté par le botaniste écossais Robert Brown en 1827. Ce phénomène aléatoire lié à l'agitation moléculaire reçut par la suite le nom de mouvement brownien. Sa description mathématique comme un processus stochastique a captivé l'attention des physiciens et mathématiciens depuis plus d'un siècle. Il intervient dans de très nombreux modèles en physique, chimie, biologie, sciences économiques et mathématiques financières. Le mouvement brownien est l'objet central du calcul des probabilités moderne : il est tout à la fois une martingale, un processus gaussien, un processus à accroissements indépendants et un processus de Markov. Ces diverses propriétés qui en font le processus stochastique par excellence, sont présentées dans cet ouvrage avec les deux outils qu'il permet de développer : l'intégrale d'Itô et la notion d'équation différentielle stochastique. Ce livre s'adresse à tous ceux et celles qui recherchent une introduction rapide et rigoureuse aux méthodes du calcul stochastique, en particulier aux étudiants des master de mathématiques, aux élèves des grandes écoles scientifiques ainsi qu'aux candidats à l'agrégation. Nous y avons inclus des exercices de difficulté variée, corrigés en fin de volume, pour en faciliter la lecture et l'utilisation comme outil pédagogique.}
      \field{journaltitle}{Hermann}
      \field{month}{9}
      \field{title}{Mouvement brownien et calcul d'{Itô}-{Léonard} {Gallardo}-{Editions} {Hermann}}
      \field{urlday}{16}
      \field{urlmonth}{4}
      \field{urlyear}{2025}
      \field{year}{2008}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://www.editions-hermann.fr/livre/mouvement-brownien-et-calcul-d-ito-leonard-gallardo
      \endverb
      \verb{url}
      \verb https://www.editions-hermann.fr/livre/mouvement-brownien-et-calcul-d-ito-leonard-gallardo
      \endverb
    \endentry
    \entry{ProbabilisticMachineLearning}{misc}{}{}
      \name{author}{1}{}{%
        {{hash=b1769bbaa2660cbedf14832523ae4d85}{%
           family={Murphy},
           familyi={M\bibinitperiod},
           given={K},
           giveni={K\bibinitperiod}}}%
      }
      \strng{namehash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{fullhash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{fullhashraw}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{bibnamehash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{authorbibnamehash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{authornamehash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{authorfullhash}{b1769bbaa2660cbedf14832523ae4d85}
      \strng{authorfullhashraw}{b1769bbaa2660cbedf14832523ae4d85}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{Probabilistic Macine Learning Advanced Topics}
      \field{year}{2023}
      \verb{urlraw}
      \verb https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/
      \endverb
      \verb{url}
      \verb https://mitpress.mit.edu/9780262048439/probabilistic-machine-learning/
      \endverb
    \endentry
    \entry{rasmussen_gaussian_2008}{book}{}{}
      \name{author}{2}{}{%
        {{hash=58d90ed7b7348c7a5a9b4a2a8f46df7b}{%
           family={Rasmussen},
           familyi={R\bibinitperiod},
           given={Carl\bibnamedelima Edward},
           giveni={C\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=d34fdfcbfcb5412397b946351370db22}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Christopher\bibnamedelimb K.\bibnamedelimi I.},
           giveni={C\bibinitperiod\bibinitdelim K\bibinitperiod\bibinitdelim I\bibinitperiod}}}%
      }
      \list{language}{1}{%
        {eng}%
      }
      \list{location}{1}{%
        {Cambridge, Mass.}%
      }
      \list{publisher}{1}{%
        {MIT Press}%
      }
      \strng{namehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{fullhash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{fullhashraw}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{bibnamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authorbibnamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authornamehash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authorfullhash}{f0ccf4a09a3a6a91719405724e3ae5da}
      \strng{authorfullhashraw}{f0ccf4a09a3a6a91719405724e3ae5da}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{edition}{3. print}
      \field{isbn}{9780262182539}
      \field{series}{Adaptive computation and machine learning}
      \field{title}{Gaussian processes for machine learning}
      \field{year}{2008}
    \endentry
    \entry{sarkka_applied_2019}{book}{}{}
      \name{author}{2}{}{%
        {{hash=89f4b98fe43f3c9f7ffdc081904d4781}{%
           family={Särkkä},
           familyi={S\bibinitperiod},
           given={Simo},
           giveni={S\bibinitperiod}}}%
        {{hash=4592dc3c98e842594b9416db6b6470ae}{%
           family={Solin},
           familyi={S\bibinitperiod},
           given={Arno},
           giveni={A\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge}%
      }
      \list{publisher}{1}{%
        {Cambridge University Press}%
      }
      \strng{namehash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{fullhash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{fullhashraw}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{bibnamehash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{authorbibnamehash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{authornamehash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{authorfullhash}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \strng{authorfullhashraw}{3ac4cf0937ba13a39c2fb845cfc9d191}
      \field{sortinit}{S}
      \field{sortinithash}{b164b07b29984b41daf1e85279fbc5ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Stochastic differential equations are differential equations whose solutions are stochastic processes. They exhibit appealing mathematical properties that are useful in modeling uncertainties and noisy phenomena in many disciplines. This book is motivated by applications of stochastic differential equations in target tracking and medical technology and, in particular, their use in methodologies such as filtering, smoothing, parameter estimation, and machine learning. It builds an intuitive hands-on understanding of what stochastic differential equations are all about, but also covers the essentials of Itô calculus, the central theorems in the field, and such approximation schemes as stochastic Runge–Kutta. Greater emphasis is given to solution methods than to analysis of theoretical properties of the equations. The book's practical approach assumes only prior understanding of ordinary differential equations. The numerous worked examples and end-of-chapter exercises include application-driven derivations and computational assignments. MATLAB/Octave source code is available for download, promoting hands-on work with the methods.}
      \field{isbn}{9781316510087}
      \field{series}{Institute of {Mathematical} {Statistics} {Textbooks}}
      \field{title}{Applied {Stochastic} {Differential} {Equations}}
      \field{urlday}{23}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{year}{2019}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1017/9781108186735
      \endverb
      \verb{urlraw}
      \verb https://www.cambridge.org/core/books/applied-stochastic-differential-equations/6BB1B8B0819F8C12616E4A0C78C29EAA
      \endverb
      \verb{url}
      \verb https://www.cambridge.org/core/books/applied-stochastic-differential-equations/6BB1B8B0819F8C12616E4A0C78C29EAA
      \endverb
    \endentry
    \entry{zhu_markovian_2023}{misc}{}{}
      \name{author}{3}{}{%
        {{hash=0fd257d8965b0398c2a4c16d8fc8bdd5}{%
           family={Zhu},
           familyi={Z\bibinitperiod},
           given={Harrison},
           giveni={H\bibinitperiod}}}%
        {{hash=1bea60c4821d47dc588222147ad3c3f4}{%
           family={Rodas},
           familyi={R\bibinitperiod},
           given={Carles\bibnamedelima Balsells},
           giveni={C\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=400e4d5630f4e518634cc6e80b62b62f}{%
           family={Li},
           familyi={L\bibinitperiod},
           given={Yingzhen},
           giveni={Y\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {arXiv}%
      }
      \strng{namehash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{fullhash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{fullhashraw}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{bibnamehash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{authorbibnamehash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{authornamehash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{authorfullhash}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \strng{authorfullhashraw}{ffe0a4b0a1025c2d9fd51b739f066dc5}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Sequential VAEs have been successfully considered for many high-dimensional time series modelling problems, with many variant models relying on discrete-time mechanisms such as recurrent neural networks (RNNs). On the other hand, continuous-time methods have recently gained attraction, especially in the context of irregularly-sampled time series, where they can better handle the data than discrete-time methods. One such class are Gaussian process variational autoencoders (GPVAEs), where the VAE prior is set as a Gaussian process (GP). However, a major limitation of GPVAEs is that it inherits the cubic computational cost as GPs, making it unattractive to practioners. In this work, we leverage the equivalent discrete state space representation of Markovian GPs to enable linear time GPVAE training via Kalman filtering and smoothing. For our model, Markovian GPVAE (MGPVAE), we show on a variety of high-dimensional temporal and spatiotemporal tasks that our method performs favourably compared to existing approaches whilst being computationally highly scalable.}
      \field{month}{8}
      \field{note}{arXiv:2207.05543}
      \field{title}{Markovian {Gaussian} {Process} {Variational} {Autoencoders}}
      \field{urlday}{23}
      \field{urlmonth}{7}
      \field{urlyear}{2025}
      \field{year}{2023}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.48550/arXiv.2207.05543
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/2207.05543
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/2207.05543
      \endverb
      \keyw{Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning}
    \endentry
  \enddatalist
  \missing{mva_kernel_class}
\endrefsection
\endinput

