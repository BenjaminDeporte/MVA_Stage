{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80222970",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 28px; color: black; font-weight: bold;\">\n",
    "Basics on Neural Latent SDE\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38425f08",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Biblio\n",
    "</div>\n",
    "\n",
    "Neural ODEs:\n",
    "\n",
    "**Neural ODEs (https://arxiv.org/abs/1806.07366) (2019)** : introduction of the Neural ODE as the continuous-time limit of a ResNet stack. Presentation of the use of the adjoint sensitivity method. Seminal paper for Neural ODE.\n",
    "\n",
    "**Latent ODEs for Irregularly-Sampled Time Series (https://arxiv.org/abs/1907.03907) (2019)** : Evolution of the Neural ODE model towards a Neural ODE RNN model, where the approximate posterior is built with a RNN on past observations.\n",
    "\n",
    "Neural SDEs:\n",
    "\n",
    "**SDE Matching: Scalable and Simulation-Free Training of Latent Stochastic Differential Equations (https://arxiv.org/abs/2502.02472 , 2025)** : good background section (#2) to explain Neural SDE. Propose a new method SDE matching, inspired by score and flow matching, vs the adjoint sensivity method. SDE matching is claimed to be more efficient to compute gradients and train latent SDEs.\n",
    "\n",
    "**Scalable Gradients for Stochastic Differential Equations (https://arxiv.org/abs/2001.01328) (2020)** : generalization of the adjoint sensitivity method to SDEs. Combination with gradient-based stochastic variational inference for infinite-dimension VAEs.\n",
    "\n",
    "**Neural SDEs (https://www.researchgate.net/publication/333418188_Neural_Stochastic_Differential_Equations) (2019)** : link between infinitely deep residual networks and solutions to stochastic differential equations\n",
    "\n",
    "**Stable Neural SDEs in analyzing irregular time series data (https://arxiv.org/abs/2402.14989) (2025)** : points to the necessity of careful design of the drift and diffusion neural nets in latent SDEs. Introduces three latent SDEs models with performance guarantees.\n",
    "\n",
    "**Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations (https://arxiv.org/abs/2412.12112) (2024)** : application of neural SDEs to a biological use case (brain activity). Details the model, architecture, ELBO/loss computation. Takes into account inputs/commands in the model. \n",
    "\n",
    "General/Misc:\n",
    "\n",
    "**Efﬁcient gradient computation for dynamical models (https://www.fil.ion.ucl.ac.uk/~wpenny/publications/efficient_revised.pdf) (2014)** : summary of finite difference method, forward sensitivity method, adjoint sensitivity method, to compute gradients of a functional cost function. Applies to Neural ODEs training.\n",
    "\n",
    "**Cyclical Annealing Schedule: A Simple Approach to Mitigating KL Vanishing (https://arxiv.org/abs/1903.10145) (2019)** : explanation of the posterior collapse/KL vanishing problem, introduces different KL annealing schedules for VAE training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d885ce",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Code : torchsde library by Google Research\n",
    "</div>\n",
    "\n",
    "https://github.com/google-research/torchsde\n",
    "\n",
    "[1] Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, David Duvenaud. \"Scalable Gradients for Stochastic Differential Equations\". International Conference on Artificial Intelligence and Statistics. 2020. [arXiv]\n",
    "\n",
    "[2] Patrick Kidger, James Foster, Xuechen Li, Harald Oberhauser, Terry Lyons. \"Neural SDEs as Infinite-Dimensional GANs\". International Conference on Machine Learning 2021. [arXiv]\n",
    "\n",
    "[3] Patrick Kidger, James Foster, Xuechen Li, Terry Lyons. \"Efficient and Accurate Gradients for Neural SDEs\". 2021. [arXiv]\n",
    "\n",
    "[4] Patrick Kidger, James Morrill, James Foster, Terry Lyons, \"Neural Controlled Differential Equations for Irregular Time Series\". Neural Information Processing Systems 2020. [arXiv]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4bf9",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Basic manipulations of the torchsde library\n",
    "</div>\n",
    "\n",
    "See also https://github.com/google-research/torchsde/blob/master/examples/demo.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "915d2c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchsde\n",
    "from torchdiffeq import odeint, odeint_adjoint\n",
    "\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27c0d8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "828eb904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Ti\n",
      "Total GPU Memory: 11.8 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.FloatTensor\n",
    "\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f28af6",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Verifying the home made calculation of KL path : Ok à un facteur 2 près...\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a24e19af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentSDE(nn.Module):\n",
    "    \n",
    "    def __init__(self, prior_theta=10.0, posterior_theta=23.0, prior_mu=1.0, posterior_mu=2.0, sigma=0.5):\n",
    "        super().__init__()\n",
    "        self.noise_type=\"diagonal\"\n",
    "        self.sde_type=\"ito\"\n",
    "        \n",
    "        # prior drift\n",
    "        self.register_buffer(\"prior_theta\", torch.tensor(prior_theta))\n",
    "        self.register_buffer(\"prior_mu\", torch.tensor(prior_mu))\n",
    "        self.register_buffer(\"sigma\", torch.tensor(sigma))\n",
    "        \n",
    "        # posterior drift\n",
    "        self.posterior_theta = nn.Parameter(torch.tensor(posterior_theta), requires_grad=True)\n",
    "        self.posterior_mu = nn.Parameter(torch.tensor(posterior_mu), requires_grad=True)\n",
    "        \n",
    "    # approx posterior drift\n",
    "    def f(self,t,z):\n",
    "        if t.dim()==0:\n",
    "            t = torch.full_like(z, fill_value=t)\n",
    "        return self.posterior_theta*(self.posterior_mu - z)\n",
    "        \n",
    "    # prior drift\n",
    "    def h(self,t,z):\n",
    "        if t.dim()==0:\n",
    "            t = torch.full_like(z, fill_value=t)\n",
    "        return self.prior_theta*(self.prior_mu - z)\n",
    "        \n",
    "    # shared diffusion\n",
    "    def g(self,t,z):\n",
    "        if t.dim()==0:\n",
    "            return self.sigma.repeat(z.size(0), 1)\n",
    "        else:\n",
    "            return self.sigma * torch.ones((t.size(0),z.size(1))).to(device)\n",
    "        \n",
    "sde = LatentSDE().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c4ecdfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times : torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "LENGTH = 100\n",
    "t_start = 0.0\n",
    "t_end = 1.0\n",
    "times = torch.linspace(t_start,t_end,LENGTH).to(device)\n",
    "print(f\"times : {times.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2742c462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z0s : torch.Size([30, 1])\n",
      "zs : torch.Size([100, 30, 1])\n",
      "logqp : torch.Size([99, 30])\n",
      "KL via logqp : 279.9403381347656\n"
     ]
    }
   ],
   "source": [
    "# compute logp\n",
    "K = 30\n",
    "z0s = torch.zeros((K,1)).to(device)\n",
    "print(f\"z0s : {z0s.shape}\")\n",
    "\n",
    "# compute SDEs\n",
    "zs, logqp = torchsde.sdeint(sde, z0s, times, method=\"euler\", dt=1e-3, logqp=True)\n",
    "\n",
    "# compute KL by averaging over batch, summing over time\n",
    "kl_path_1 = logqp.mean(dim=1).sum()\n",
    "\n",
    "# report\n",
    "print(f\"zs : {zs.shape}\")\n",
    "print(f\"logqp : {logqp.shape}\")\n",
    "print(f\"KL via logqp : {kl_path_1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0af281c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home made KL : torch.Size([99, 30, 1])\n",
      "KL via home made : 558.3657836914062\n"
     ]
    }
   ],
   "source": [
    "# compute logqp home made\n",
    "epsilon = 1e-6\n",
    "\n",
    "# compute prior drifts\n",
    "prior_drifts = sde.h(times, zs)\n",
    "posterior_drifts = sde.f(times, zs)\n",
    "diff = sde.g(times, zs).unsqueeze(-1)\n",
    "diff2 = torch.where(diff.abs().detach() > epsilon, diff, torch.full_like(diff, fill_value=epsilon) * diff.sign())\n",
    "deltas = torch.div(posterior_drifts - prior_drifts, diff2)**2\n",
    "approx_int = torch.stack( [1/2 * (times[i+1]-times[i]) * (deltas[i,:,:] + deltas[i+1,:,:]) for i in range(times.shape[0]-1) ] )  # n_steps-1 x K x 1\n",
    "\n",
    "print(f\"Home made KL : {approx_int.shape}\")\n",
    "kl_path_2 = approx_int.mean(dim=1).sum()\n",
    "print(f\"KL via home made : {kl_path_2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "torchy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
